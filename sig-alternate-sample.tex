% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}

\usepackage{fancyvrb}

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Is MFS Strongly Correlated with faulty code?
\titlenote{(Produces the permission block, and
copyright information). For use with
SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{5} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Xintao Niu\\
       \affaddr{State Key Laboratory for Novel Software Technology}\\
       \affaddr{Nanjing University}\\
       \affaddr{China, 210023}\\
       \email{niuxintao@gmail.com}
% 2nd. author
\alignauthor
Changhai Nie\\
       \affaddr{State Key Laboratory for Novel Software Technology}\\
       \affaddr{Nanjing University}\\
       \affaddr{China, 210023}\\
       \email{changhainie@nju.edu.cn}
% 3rd. author
\alignauthor
Xiaoyin Wang \\
       \affaddr{Department of Computer}\\
       \affaddr{Science}\\
       \affaddr{The University of }\\
       \affaddr{Texas at San Antonio}\\
 %      \affaddr{China, 211171}\\
%       \email{xujiaxi@njxzc.edu.cn} \\
       \email{Xiaoyin.Wang@utsa.edu}
 % use '\and' if you need 'another row' of author names
 \and
% 4th. author
\alignauthor
Hareton Leung \\
       \affaddr{Department of computing}\\
       \affaddr{Hong Kong Polytechnic University}\\
       \affaddr{Kowloon, Hong Kong}\\
       \email{hareton.leung@polyu.edu.hk}
\alignauthor
Jeff Lei \\
       \affaddr{Department of Computer}\\
       \affaddr{Science and Engineering }\\
       \affaddr{The University of Texas at Arlington}\\
%       \affaddr{Arlington, Texas}\\
       \email{ylei@cse.uta.edu}
%% 6th. author
%\alignauthor Charles Palmer\\
%       \affaddr{Palmer Research Laboratories}\\
%       \affaddr{8600 Datapoint Drive}\\
%       \affaddr{San Antonio, Texas 78229}\\
%       \email{cpalmer@prl.com}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
%\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Combinatorial Testing (CT) is an effective technique for testing the interactions of factors in the Software Under Test(SUT). Most works in CT focus on the method itself, e.g., how to generate test cases, model the inputs, or handle the constraints of the inputs. Few of the works consider the justification of CT, i.e., is detecting and identifying the failure-inducing interactions really useful and helpful to code-level fault diagnosis? In this paper, we novelty studied the relationship between the failure-inducing interactions and code which causes the failure. Specifically, based on symbolic execution, we firstly obtain the guaranteed code of the corresponding failure-inducing interactions, i.e., those program entities which are directly affected by these interactions. And then we will compared these guaranteed code with those real faulty code to see whether there exists any associations between failure-inducing interactions with these real faulty code. Our empirical studies based on 5 real subjects showed that the failure-inducing interactions are strongly correlated with faulty code in the SUT.

\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011102.10011103</concept_id>
<concept_desc>Software defect analysis~Software testing and debugging</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software defect analysis~Software testing and debugging}
%\ccsdesc{Reliability Verification~a}
%\ccsdesc[100]{Networks~Network reliability}

%
%\category{D.2.5}{Software Engineering}{Testing and debugging}[Debugging aids,testing tools]
%
%\terms{Reliability, Verification}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{Software Testing, Combinatorial Testing, Fault localization, Failure-inducing interactions, Guaranteed code}


\section{Introduction}
Modern software is becoming more and more complex. To test such software is challenging, as the candidate factors that can influence the system's behaviour, e.g., configuration options, system inputs, message events, are enormous. Even worse, the interactions between these factors can also crash the system, e.g., the incompatibility problems. In consideration of the scale of the industrial software, to test all the possible interactions of all the factors (we call them the interaction space) is not feasible, and even if it is possible, it is not wise to test all the interactions because most of them do not provide any useful information.
%Further more, it is a standard . airline

Many empirical studies show that, in real software systems, the effective interaction space, i.e., targeting fault detection, makes up only a small proportion of the overall interaction space \cite{kuhn2002investigation,kuhn2004software}. What's more, the number of factors involved in these effective interactions is relatively small, of which 4 to 6 is usually the upper bounds\cite{kuhn2002investigation}. With this observation, applying Combinatorial testing(CT) in practice is appealing, as it is proven to be effective to detect the interaction faults in the system.

CT tests software with a elaborate test suite which checks all the required parameter value combinations, and after detecting some failures by this test suite, it then identify the failure-inducing interactions, or more formally, failure-causing schemas (MFS) in the SUT. Most works in CT focus on the method itself, e.g, to design smaller test suite with the same interaction coverage \cite{cohen1997aetg,cohen2003augmenting,lei2008ipog,jia2015learning}, or to identify the MFS more accurately \cite{martinez2009locating,nie2011minimal,zhang2011characterizing,niu2013identifying}. Few of the works consider the following question:

\emph{Is detecting and identifying the MFS really useful and helpful to code-level fault diagnosis?}

To analyse this question is important and necessary, because it will build the relationship between MFS and faulty code, which is the foundation to apply CT on code-level debugging. In this paper, we try to answer this question by studying the \emph{guaranteed code} of interactions. The \emph{guaranteed code} of a interaction is the program entities (e.g., statements, branches, blocks, etc.) which are directly \emph{affected} by the interaction according to the previous study \cite{reisner2010using}. Obtaining the guaranteed code of an interaction can help us understand how this interaction influence on the behaviour of the program under test. Furthermore, analysing the guaranteed code of the MFS can offer us an insight into the extent to which the MFS is related to the cause of the failure; based on which, we can learn whether detecting and identifying the MFS can facilitate the debugging, as well as bug fixing.

%To compute the MFS of the SUT,
%
%With respect to the guaranteed code.
%
%To compute the guaranteed code of the interaction, We need to utilize symbolic execution. Specifically, we accumulated added on symbolic, and find those guaranteed code. We define the .

%
There are many techniques to compute the MFS in CT. In this paper, we adopts our previous method proposed in \cite{nie2011minimal}, which is one of the most common MFS identification technique in CT. With respect to guaranteed code, we follow the steps which are original proposed in \cite{reisner2010using}, which firstly utilizes symbolic execution tool to search possible paths for different values assigned to the input parameters, and then calculates the guaranteed code for each possible interaction based on these paths. One difference from study in \cite{reisner2010using} is that we only need to compute the guaranteed code for the MFS we identified in the SUT, instead of all the possible interactions. After obtaining the MFS and corresponding guaranteed codes, we will evaluate the correlation between MFS and faulty code.

We have design several empirical studies on 5 open-source software subjects. These studies considers several different aspects (e.g., the degree of MFS, the types of faults) of the relationships between MFS and faulty code. Our results suggests that: 1) The MFS does relate to the faulty code to some extent; 2) For different types of faults, the correlation between MFS and faulty code varies;  3) The input model of the program under test significantly impacts on this correlation.

%These faults include injected and real ones. We first model their input and computed the MFS and also the guaranteed code of these MFS. We observe their relationships between these code with those real faulty statement. One observation in our case study shows that .  Besides this, we also compare . The results shows that .

%\textbf{Contributions:}

The remaindering of this paper are organised as follows: Section \ref{sec:pre} gives the preliminaries about Combinatorial testing (especially MFS-related), and basic definitions about Guaranteed code. Section \ref{sec:research} proposes three research questions that needs to be handled in this paper. Section \ref{sec:subjects} introduces the subjects on which our experiments are conducted on. Section \ref{sec:results} shows the results as well as the analysis. Section \ref{sec:related} discusses the related works.  Section \ref{sec:conclusion} concludes this paper.

\section{Preliminary and Formal Model}\label{sec:pre}
This section presents some formal descriptions about MFS and guaranteed code.

\subsection{Basic definitions about CT}\label{sec:pre:ct}

Assume that the Software Under Test (SUT) is influenced by \emph{n} parameters, and each parameter $p_{i}$ can take the values from the finite set $V_{i}$, $|V_{i}|$ = $a_{i}$ ($i$ = 1,2,..n). We will give some basic definitions which are related to failure-inducing interactions in CT.

\newdef{definition}{Definition}
\begin{definition}
A \emph{test case} of the SUT is a tuple of \emph{n} values, one for each parameter of the SUT. It is denoted as  ($v_{1}$, $v_{2}$,...,$v_{n}$), where $v_{1}\in V_{1}$, $v_{2} \in V_{2}$ ... $v_{n} \in V_{n}$.
\end{definition}
%a \emph{n}-tuple

In practice, these parameters in the test case can represent many factors, such as input variables, run-time options, building options or various combination of them. We need to execute the SUT with these test cases to ensure the correctness of the behaviour of the SUT.

%\begin{definition}
%We consider any abnormally executing test case as a \emph{fault}. It can be a thrown exception, compilation error, assertion failure or constraint violation. When faults are triggered by some test cases, it is desired to figure out which parameter values in these test cases are the failure-inducing ones.
%Some subsets of these test cases should be analysed.
%\end{definition}


\begin{definition}
For the SUT, the \emph{n}-tuple (-,$v_{x_{1}}$,...,$v_{x_{k}}$,...)is called a \emph{k}-degree \emph{schema} ($0 < k \leq n $) when some k parameters have fixed values and other irrelevant parameters are represented as "-".
\end{definition}

For example, the tuple (-, 4, 4, -) is a 2-degree schema. In effect a test case itself is a k-degree \emph{schema}, when k = n. Furthermore, if a test case contains a \emph{schema}, i.e., every fixed value in the schema is in this test case, we say this test case \emph{contains} the \emph{schema}.
%, which can be denoted as $k-value\  combination \in T$

Note that the schema is a formal description of the interaction between parameter values we discussed before.

\begin{definition}
Let $c_{l}$ be a \emph{l}-degree schema, $c_{m}$ be an \emph{m}-degree schema in SUT and $l < m$. If all the fixed parameter values in $c_{l}$ are also in $c_{m}$, then $c_{m}$ \emph{subsumes} $c_{l}$. In this case we can also say that $c_{l}$ is a \emph{sub-schema} of $c_{m}$ and $c_{m}$ is a \emph{super-schema} of $c_{l}$, which can be denoted as $c_{l} \prec  c_{m}$.
\end{definition}

For example,  the 2-degree schema (-, 4, 4, -) is a sub-schema of the 3-degree schema (-, 4, 4, 5), that is, (-, 4, 4, -) $\prec$ (-, 4, 4, 5).

\begin{definition}
If all test cases that contain a schema, say $c$, trigger a particular fault, say $F$, then we call this schema $c$ the \emph{faulty schema} for $F$. Additionally, if none of sub-schema of $c$ is the \emph{faulty schema} for $F$, we then call the schema $c$ the \emph{minimal failure-causing schema (MFS)} \cite{nie2011minimal} for $F$.

%Based on this, if a test case $t$ hit such a failure-inducing combination, say $c(F)$, it should trigger the fault $F$, for which the test case can be put as $t(F)$
\end{definition}

Note that MFS is identical to the failure-inducing interaction discussed previously. In this paper, the terms \emph{failure-inducing interactions} and \emph{MFS} are used interchangeably.
%Figuring the MFS out helps to identify the root cause of a failure and thus facilitate the debugging process.

\subsection{Identification of MFS}\label{sec:pre:mfs}
When a test case fails during test case, we are still far from figuring out the MFS \cite{colbourn2008locating,martinez2008algorithms,martinez2009locating}, as we do not know exactly which schemas in the failed test cases should be responsible for the failure. For example, if test case (0, 0, 0, 0) failed during testing, there are six 2-degree candidate failure-inducing schemas, which are (0, 0, -, -), (0, -, 0, -), (0, -, -, 0) , (-, 0, 0, -), (-, 0, -, 0), (-, -, 0, 0), respectively. Without
additional information, it is difficult to figure out the specific schemas in this suspicious set that caused the failure. Considering that the failure can be triggered by schemas with other degrees, e.g., (0, -, -, -) or (0, 0, 0, -), the problem of MFS identification becomes more complicated.

In fact, for a failing test case ($v_{1},v_{2},...,v_{n}$), there can be at most $2^{n} - 1$ possible schemas for the MFS. Hence, more test cases should be generated to identify the MFS. Next, we will show how MFS identification processes with an example.

Consider an program like Fig \ref{toy-program}, which contains four integer parameters: a, b, c, and d. With respect to applying combinatorial testing, we need to first build a input model for this program. For simplicity, let a, b, c and d can take on the following values:  $v_{a} = \{ 1, 2\}$, $v_{b} = \{ 0, 1\}$, $v_{c} = \{ 1, 2\}$, and $v_{d} = \{ 0, 1\}$, respectively. Assume that through testing of this program, we find a test case, e.g., (a = 1, b = 0, c = 1, d = 1), will trigger an arithmetic exception. Then we will describe how CT identify the MFS in this test case.

%taking square root of a negative number
\begin{figure}
\begin{Verbatim}[numbers=left,xleftmargin=5mm]
public float foo(int a, int b, int c, int d){
  int x, y, z;
  x = 4;
  if(a < 2){
    y = 3;
    if(c < 2){
            z =  Math.sqrt(y - x);
            return z / y;
    }
    else
        return x + y;
  }
  else{
    y = 2;
    if(b < 1){
        if(d < 1)
            return y / (x + y);
        else
            return x / (x + y);
    }
    else
        return y * x;
  }
}
\end{Verbatim}
\caption{A simple program \emph{foo} with four input parameters}
\label{toy-program}
\end{figure}

A typical MFS identification process is shown in Table \ref{ofot-identify}. In this table, test case $t$ represents the failing test case we aforementioned. To identify the MFS, we mutate one factor of \emph{t} one time to generate new test cases: $t_{1}$ -- $t_{4}$.  It turns out that test case $t_{1}$ passed, which indicates that this test case break the MFS in the original test case \emph{t}. So (1, -, -, -) should be a failure-causing factor. Similarly, we can also conclude that (-, -, 1, -) is another failure-inducing factor because of the pass of $t_{3}$. Considering that all the other test cases failed, which means no other failure-inducing factors were broken, therefore, the MFS in \emph{t} is (1, -, 1, -).

\begin{table}[ht]
\caption{OFOT example}
\label{ofot-identify}
\center
\begin{tabular}{llllll}
 \hline
\multicolumn{5}{c}{\bfseries Original test case} & \bfseries Outcome \\  \hline
 $t$ & \multicolumn{4}{l}{1 \ \ \ \ 0 \ \ \ \  1 \ \ \ \ 1 } & Fail \\
 \hline
\multicolumn{5}{c}{\bfseries Additional  test cases} &  \\  \hline
$t_{1}$ &\multicolumn{4}{l}{2  \ \ \ \  0 \ \ \ \  1 \ \ \ \ 1 }& Pass \\
$t_{2}$ &\multicolumn{4}{l}{1  \ \ \ \  1 \ \ \ \  1 \ \ \ \ 1 } & Fail \\
$t_{3}$ &\multicolumn{4}{l}{1  \ \ \ \  0 \ \ \ \  2 \ \ \ \ 1 } & Pass \\
$t_{4}$ &\multicolumn{4}{l}{1  \ \ \ \  0 \ \ \ \  1 \ \ \ \ 0 } & Fail \\
 \hline
\end{tabular}
\end{table}

This identification process mutate one factor of the original test case at a time to generate extra test cases. Then according to the outcome of the test cases execution result, it will identify the MFS of the original failing test cases. It is called the OFOT method \cite{nie2011minimal}, which is a well-known MFS identification method in CT. In this paper, we will focus on this identification method.
%It should be noted that the following proposed CT framework can be easily applied to other MFS identification methods.


\subsection{Formal description about guaranteed code}\label{sec:pre:guar}
Although MFS are the failure-inducing parts of the failing test input for the SUT, however, we still cannot directly utilize it for fault localization, because they do not provide any code-level information. For this, we need to build the relationship between input schemas and program entities.

Let $\mathcal{P}$ be a path in the SUT. Then let $Cov(\mathcal{P})$ = $< s_{1}, s_{2}, s_{3},... s_{n} >$ be the program entities that covered by path $\mathcal{P}$. A program entity can be a statement, block, edges, etc. In this paper, we mainly focus on statements; note that other types of structure coverage can also be applied \cite{song2012itree}. Let $Pcon(\mathcal{P})$ = $<pc_{1}, pc_{2}, pc_{3}, ... pc_{k}>$ be the path conditions that are encountered by path $\mathcal{P}$.  As an example, consider the program list in Fig \ref{toy-program}. It is easy to find that it has five possible paths, which forms the execution tree in Fig \ref{foo-tree}.

\begin{figure}
 \includegraphics[width=3.4in]{foo_tree.eps}
\caption{The execution tree of program \emph{foo}}
\label{foo-tree}
\end{figure}

In this tree, a rhombus node represents a path condition, and a rectangle represents the statement. Note that those consecutive statements are included in one rectangle. From this figure, we can list the paths, their covered entities, and their path conditions, which are explicitly shown in Table \ref{cov-con-foo}.

\begin{table}[ht]
\caption{Paths, covered entities, and conditions of \emph{foo}}
\label{cov-con-foo}
\center
\begin{tabular}{lll}
 \hline
\bfseries ID & \bfseries Covered Entities & \bfseries Path Conditions \\  \hline
 $1$ & S2, S3, S5, S7, S8& $a < 2$,  $c < 2$\\
 $2$ & S2, S3, S5, S11 & $a < 2$,  $\neg (c < 2)$\\
 $3$ & S2, S3, S14, S17& $\neg (a < 2)$,  $b < 1$, $d < 1$\\
 $4$ & S2, S3, S14, S19& $\neg (a < 2)$,  $b < 1$, $\neg (d < 1)$\\
 $5$ & S2, S3, S14, S22& $\neg (a < 2)$,  $\neg (b < 1)$\\
 \hline
\end{tabular}
\end{table}


Next we will give some important definitions that are related to the guaranteed code.
%For simplicity, we next use notation $c^\sharp$ indicates a logic expression meaning that the parameter values are equal to those in schema $c$.

\begin{definition}
For a schema $c$, and a path $\mathcal{P}$, if all the path conditions in this path can be satisfied when given an input that contains this schema, we call $\mathcal{P}$ the \emph{Consistent path} of schema $c$, which can be denoted as $SAT(Pcon(\mathcal{P}) \bigwedge c)$.
\end{definition}

In fact, to judge whether a path is the \emph{consistent path} of one schema, is to find whether exist an input that contains this schema and follows all the path conditions of this path. Taking the program \emph{foo} for example, Path 1 is the consistent path of schema (1, -, -, -), because $( a < 2 $ $\wedge $ $c < 2 $ $\wedge $ $a = 1 )$  $= true $ can be solved, e.g., input (1, 1, 1, 1) is such a result.

Based on this definition, we use the notation $c^{\sharp}$ to represent the set of all the consistent paths of schema $c$. Then we will give the formal definition of weak guaranteed of one schema.

\begin{definition}
For a schema $c$, the weak guaranteed code of $c$, i.e., $wg(c)$, is the intersection of program entities covered by its consistent paths. Formally, $wg(c)$ = $\bigcap_{\mathcal{P} \in c^{\sharp}} Cov(\mathcal{P})$.

\end{definition}

To better illustrate the weak guaranteed code of schemas, we list some schemas in the program \emph{foo}, their consistent paths and weak guaranteed code in Table \ref{weg-foo}. For example, for schema (1, -, -, -), its consistent paths are Path 1 and Path 2, because we can find inputs that contain this schema and satisfy all the path conditions of them. For example, (1, 1, 1, 1) can go through Path 1 and (1, 1, 2, 1) can go through Path 2. These two inputs both contain the schema of (1, -, -, -). Hence, the weak guaranteed code of $(1, -, -, -)$ are the intersection of the entities covered by these two paths, i.e., S2, S3, and S5.

Note that there is one special schema (-, -, -, -), listed in this Table, it is the sub-schemas of all the other schemas of program \emph{foo}. What's more, it is consistent with all the paths of program \emph{foo}, and hence its weak guaranteed code are S2 and S3, which are the common code of all the paths.

\begin{table}[ht]
\caption{Weak guaranteed code of some schemas of \emph{foo}}
\label{weg-foo}
\center
\begin{tabular}{lll}
 \hline
\bfseries Schema & \bfseries $c^{\sharp}$ & \bfseries $wg(c)$\\  \hline
 $(1, -, -, -)$ & Path 1, 2& S2, S3, S5 \\
 $(1, -, 1, -)$ & Path 1 & S2, S3, S5, S7, S8\\
 $(1, -, 2, -)$ & Path 2 & S2, S3, S5, S11\\
 $(2, -, -, -)$ & Path 3, 4, 5& S2, S3, S14\\
 $(2, 0, -, -)$ & Path 3, 4& S2, S3, S14\\
 $(2, 0, -, 1)$ & Path 4& S2, S3, S14, S19\\
 $(2, 1, -, -)$ & Path 5& S2, S3, S14, S22\\
 $(-, -, 1, -)$ & Path 1, 3, 4, 5 & S2, S3 \\
 $(-, -, -, -)$ & Path 1, 2, 3, 4, 5 & S2, S3 \\
 \hline
\end{tabular}
\end{table}

Above all, we can observe that, the weak guaranteed code\footnote{The definition of weak guaranteed code is similar to the \emph{guarantee coverage} defined in \cite{reisner2010using}. The difference is that in this paper we do not distinguish the input and value symbolic; instead, they are handled in an unified way.} of one schema, essentially, is the maximal common code that is expected to be executed by any path which consists with this schema. In other word, this schema guarantees some code to be executed.

%\footnote{Weak guaranteed is ,
%Strong guaranteed is Note that this is the same to the interactions in paper. To let different, we }.

%Note that the definitions is one different from original defined, . which we do not consider input, instead, they are also modeled as symbolic. Based on This, all the input can be handled unified as the same way as those symbolic values.



Although the weak guaranteed code always follows with the corresponding schema, it does not necessarily indicates that the schema directly controls the weak guaranteed code. This is because, for one schema, there may exists some other schemas that may always appears with this schema. Hence, it may result in that some code are the weak guaranteed code of one schema, but these code may be actually controlled by other schemas. It is easy to learn that these ``accompanying'' schemas are the sub-schemas of one schema.

For example, with respect to program \emph{foo}, schema (1, -, 1, -) always appears with schema (1, -, -, -), hence, the weak  guaranteed code of schema (1, -, 1, -), i.e., (S2, S3, S5, S7, S8) must always contain the weak guaranteed code of schema (1, -, -, -), i.e., (S2, S3, S5).
More formally, we can conclude the relationship of the weak guaranteed code between subsuming schemas as the following proposition.

\newtheorem{proposition}{Proposition}
\begin{proposition}
Given schemas $s_{1}$, $s_{2}$, where $s_{1} \prec s_{2}$, then $wg(s_{1}) \subseteqq wg(s_{}2)$.
\end{proposition}

It is easy to prove this proposition and we will omit it. Based on this proposition, to understand which code are under the direct control of some schemas, we need to remove the influence from their subschemas.
%Considering this, it may in fact the behaviour under the sub-schema . For example, in Fig.  Hence, to totally reflect the influence behaviour of sub-schemas, we need to remove the impact from the sub-schemas.

\begin{definition}
For a schema $c$, the strong guaranteed code of $c$, i.e., $sg(c)$, is the weak guaranteed code of $c$ with removing those weak guaranteed code of its subschemas. Formally, $sg(c)$ = $wg(c) \backslash \{\bigcup_{c' \prec c} wg(c') \}$.

\end{definition}

Strong guaranteed code are the program entities that under the direct control of the corresponding schema, and hence it reflects how the schema influence on the behaviour of program. As an example, considering the schemas which we show their weak guaranteed code in Table \ref{weg-foo}. We list the weak guaranteed code of all their subschemas, and the strong guaranteed code of themselves in Table \ref{strong-foo}. For example, for schema (1, -, 1, -), its weak guaranteed code are (S2, S3, S5, S7, S8), while the weak guaranteed code of all its subschemas are: (S2, S3, S5), (S2, S3), (S2, S3) for schemas (1, -, -, -), (-, -, 1, -) and (-, -, -, -), respectively. Hence, the union of the weak guaranteed code of its subschemas are (S2, S3, S5), and the strong guaranteed code of (1, -, 1, -) are (S7, S8).

\begin{table}[ht]
\caption{Strong guaranteed code of some schemas of \emph{foo}}
\label{strong-foo}
\center
\begin{tabular}{lll}
 \hline
\bfseries Schema & \bfseries $wg\ of\ subschemas$ & \bfseries $sg(c)$\\  \hline
 $(1, -, -, -)$ & S2, S3 & S5 \\
 $(1, -, 1, -)$ & S2, S3, S5 & S7, S8\\
 $(1, -, 2, -)$ & S2, S3, S5 & S11\\
 $(2, -, -, -)$ & S2, S3 &  S14\\
 $(2, 0, -, -)$ & S2, S3, S14 &  - \\
 $(2, 0, -, 1)$ & S2, S3, S14 & S19\\
 $(2, 1, -, -)$ & S2, S3, S14 & S22\\
 $(-, -, 1, -)$ & S2, S3 & - \\
 $(-, -, -, -)$ & - & S2, S3 \\
 \hline
\end{tabular}
\end{table}


In this paper, we focus on the guaranteed code of MFS, instead of all the other schemas in the test case. With respect to the example in Fig \ref{toy-program}, it is easily to find that the weak guaranteed code and strong guaranteed code of the MFS (1, -, 1, -) are, (S2, S3, S5, S7, S8), and (S7, S8),  respectively. They are correlated to the faulty code, i.e., S7, where an exception of taking square root of a negative number will be triggered. This example, especially for the strong guaranteed code, implies that MFS is closely related to the faulty code.  However, in the real situation, we do not know whether the conclusion can still hold. As we want to know whether to detect and identify the MFS is really helpful in the code-based diagnosis, hence, we need to do more empirical studies to verify the conclusion.


%\subsection{The method to get guaranteed code}\label{sec:pre:getguar}
%
%We need to first utilize the symbolic executions.
%
%Then for the MFS we need to compute the consisient, we need a smt solver to solve this problem.
%definition guaranteed code
%
%definition minimal guaranteed code
%%

%
%In this paper, we use the symbolic execution to . The details is as follwos:




%\section{A case study}
%This section will provide an example to show how to



\section{Research questions} \label{sec:research}
To comprehensive study the correlativeness between the MFS and faulty code, we propose three research questions in the following.

\subsection{The correlation between MFS and faulty code}
As discussed before, it is important to study the relationship between MFS and faulty code. Although the simple example in Section \ref{sec:pre:guar} shows there exists a strong correlation between the guaranteed code , it is necessary to verify it on more real program subjects. Hence, it motivates our first research question, which is also the key reteach question, that is :

\textbf{Q1: Is the guaranteed code of the MFS correlated to the faulty code in real program subjects?}

To answer this question, we need to conduct studies on a batch of program subjects. In these studies, the MFS, their weak and strong guaranteed code, and the fault code should be investigated and analyzed. Another point that needs to note is how we evaluate the correlation between the guaranteed code and faulty code. Inspired by the ranking formula that is used in fault localization \cite{naish2011model,abreu2007accuracy}, in this paper, we adopts the following  formula to compute the relevance between two codes, i.e.,
 \begin{equation}\begin{aligned}\label{eq:metric}
 Correlation (A, B)= \frac{Common(A, B)}{Common(A, B) + Different(A, B)} .\end{aligned} \end{equation}

Here, $Common(A, B)$ means the number of common statements between two code blocks A and B, and $Differen$ $t (A, B)$ means the number of different statements contained in them. Based on this formula, more number of common code indicates a closer correlation, while more number of different code, on the contrary, indicates a more distinct or irrelative relationship.

% the number of blocks that we still need to inspect until we hit the faulty code. Considering we only offer one possible statement, other statements are all depend on their execution sequence. If there is many, we only obtain the sentect that hit the failure, and its distance.

\subsection{The influence of different types of faults}

According to the nature of the defect, e.g., missing construct, or wrong construct, faults can be categorized into many types. The influence of different type of fault varies widely; as a consequence, the MFS and corresponding guaranteed code of different type of fault may also varies. Hence, focusing on single type of fault may significantly impact on the generality of our study about the correlation between MFS and faulty code, which motivates the second research question:

\textbf{Q2: To which extent does different type of fault influence on the correlation between MFS and faulty code ?}

To answer this question, we need first give the the characterization and classification of software faults. According to the study \cite{duraes2006emulation}, we decide to conduct experiments on the fault types listed in Table \ref{tab_fault typles}. These faults are classified according to the three ODC \cite{chillarege1996orthogonal} classes, i.e., Assignment, Checking, and Interface faults. Note that we have omit two more types of faults which are originally listed in \cite{duraes2006emulation}, as those two types of faults are related to the design or requirement faults. For each of these faults, they are refined into three sub-types, which are based on the nature of the defects, i.e., the fault caused by missing construct, wrong construct, or superfluous part. The column ``Example'' in Table \ref{tab_fault typles} shows some samples of the corresponding type of fault.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[!ht]
  \centering
  \caption{Fault types according to nature and ODC class}
    \begin{tabular}{|c|c|c|}  \hline
    ODC class & Nature & Example \\ \hline
    Assignment & missing & A variable was not assigned a value, a variable was not initialized, etc. \\
          & wrong & A wrong value (or expression result, etc) was assigned to a variable \\
          & extraneous & A variable should not have been subject of an assignment \\  \hline
    Checking & missing & An "if" construct is missing, part of a logical condition is missing, etc. \\
          & wrong & Wrong logical expression used in a condition in branch and loop construct. \\
          & extraneous & An "if" construct is superfluous and should not be present \\  \hline
    Interface & missing & A parameter in a function call was missing; incomplete expression as used as parameter \\
          & wrong & Wrong information was passed to a function call (value, expression result, etc.) \\
          & extraneous & Surplus data is passed to a function \\  \hline
    \end{tabular}%
  \label{tab_fault typles}%
\end{table*}%

Based on the categories given in Table \ref{tab_fault typles}, we next study these faults and their corresponding MFS to observe whether there exists any distinction among different types.


\subsection{The influence of inputs model}
The last research question is raised from CT itself. It is known that inputs modeling, as the key part of CT \cite{nie2011survey}, significantly influence on the result of CT. For example, if we use the following inputs modeling : $v_{a} = \{ 0, 1 \}$, $v_{b} = \{ 0, 1\}$, $v_{c} = \{ 1, 2\}$, and $v_{d} = \{ 0, 1\}$, respectively, for the program \emph{foo} in Fig \ref{toy-program} instead of what is originally used in Section \ref{sec:pre}. Then it is easy to compute that MFS of \emph{foo} is (-, -, 1, -), which is different from the original MFS (1, -, 1, -). We can learn the new MFS is irrelevant to the parameter $a$; this is because according to the execution tree in Fig \ref{foo-tree}, no matter what $a$ is assigned to ($0$ or $1$), it can only follow $Path 1$ or $Path 2$, and hence cannot execute other paths. As a result, the fault is only depended on what $c$ is assigned to. The changes of MFS also impacts on the guaranteed code. From Table \ref{weg-foo} and \ref{strong-foo}, the weak guaranteed code of (-, -, 1, -) is $S2$ and $S3$, while there is no strong guaranteed code. This results shows that the correlation between MFS and the real faulty code ($S7$) is  very trivial, which is different from the conclusion which is based on the original inputs modeling.

Considering this, the third research question is:

\textbf{Q3: How does the inputs model of CT affect the correlation between MFS and faulty code?}


To answer question, for each program subject in our empirical studies, we design 3 different inputs model (they vary in the the number of parameters and the number of values for each parameter). Then we will obtain their MFS, guaranteed code, and the correlation between MFS and faulty code, respectively. At last, we will evaluate whether there exist any difference among these results.

%These questions are important, as

\section{Subject programs}\label{sec:subjects}

\subsection{Generating faulty programs}

\subsection{Symbolic execution of the paths}


%\section{Getting MFS and their guaranteed code}

\section{Results}\label{sec:results}

\subsection{General Correlation}

\subsection{Fault types}

\subsection{Inputs modeling }


\subsection{Threats to Validation}

There are several threats to validity in our empirical studies. 

First, our experiments are based on only 5 open-source software. More subject programs are desired to make the results more general. 

Second, the faults should be extended to more real faults, not only with those injected ones.

At last, we should increase the types of faults. For example, it is appealing to study the multiple faults, faults that relate to more than one statements.

\section{Related works}\label{sec:related}

There are several works that are related to our study, and they can be categorised into two types:

The first type of methods studies the MFS in CT. Nie \cite{nie2011minimal} firstly studied the properties of the MFS in SUT, based on which additional test cases were generated to identify them. Other approaches to identify the MFS in SUT include building a tree model \cite{yilmaz2006covering}, adaptively generating additional test cases according to the outcome of the last test case \cite{zhang2011characterizing}, ranking suspicious interactions based on some rules \cite{ghandehari2012identifying}, and using graphic-based deduction \cite{martinez2008algorithms}, among others. These approaches can be partitioned into two categories \cite{colbourn2008locating} according to how the additional test cases are generated: \emph{adaptive}--additional test cases are chosen based on the outcomes of the executed tests \cite{shi2005software,nie2011minimal,ghandehari2012identifying,niu2013identifying,zhang2011characterizing,shakya2012isolating,wang2010adaptive,li2012improved}or \emph{nonadaptive}--additional test cases are chosen independently and can be executed in parallel \cite{yilmaz2006covering,colbourn2008locating,martinez2008algorithms,martinez2009locating,zhang2012faulty}. All these works focused on input-level or configuration-level testing, i.e., their target is to identify the failure-inducing inputs or options, not the code in the program.

Besides these work, there exists two studies that focus on utilizing MFS to locate the faulty statement in the program \cite{ma2013locating,ghandehari2013fault}. The first work \cite{ma2013locating} adopted OFOT \cite{nie2011minimal} to calculate the MFS, and then compares the additional generated test cases with the original failing test cases to locate the failure-causing chains and corresponding statements. The second work \cite{ghandehari2013fault} took the ranking method \cite{ghandehari2012identifying} to identify the MFS. Then  it generated additional passing test cases based on the MFS, and ranked the suspicious statements by comparing the spectrum of the failing test case and passing test cases.

Our work differs from the first type of work in that we focus on both MFS and faulty code, not just any single object. Additionally, our work does not specifically describes how to identify MFS or how to conduct code-level fault localization, instead, we studied the correlation between them. We believe this is important and will give a guideline for how to utilize MFS for fault diagnosis.

%Combinatorial testing has been widely applied in industry \cite{kuhn2010practical}. Fault localization is an important part of CT studies. \cite{nie2011survey}, as it can facilitate debugging efforts by reducing the scope of code that is needed for inspection \cite{ghandehari2012identifying}.
%
%Shi and Nie \cite{shi2005software} presented an approach for failure revealing and failure diagnosis in CT , which first tests the SUT with a covering array, then reduces the value schemas contained in the failing test case by eliminating those appearing in the
%passing test cases.
%
%Nie's approach in \cite {nie2011minimal} first separates the faulty-possible tuples and healthy-possible tuples into two sets. Subsequently, by changing one parameter value at a time of the original test case, this approach generates extra test cases. After executing the configurations, the approach converges by reducing the number of tuples in the faulty-possible sets. Wang then \cite{wang2010adaptive} extended this approach by adaptively mutates multiple parameter values instead of only one factor at a time.
%
%Delta debugging \cite{zeller2002simplifying} proposed by Zeller is an adaptive divide-and-conquer approach to locate interaction fault. It is very efficient and has been applied to real-life software environment. Zhang et al. \cite{zhang2011characterizing} also proposed a similar approach that can identify the failure-inducing combinations that have no overlapped part efficiently.  JieLi proposed a similar approach \cite{li2012improved}.
%
%%Charles \cite{colbourn2008locating} proposed,
%
% Charles \cite{colbourn2008locating} proposed the notion of ($d$,$t$)-detecting array, which corresponds to test cases that permit the determination of MFS in the SUT, if the number of MFS $d$ and degree of MFS $t$ are known in prior.  C. Martiez \cite{martinez2008algorithms} also proposed a non-adaptive method. Their approach extends the covering array to the locating array to detect and locate interaction faults. They \cite{martinez2009locating} then extend this non-adaptive method to any degree with the theory of hyber-graphic.  C. Martiez \cite{martinez2008algorithms,martinez2009locating}proposed two adaptive algorithms. The first one needs safe value as their assumption and the second one remove this assumption when the number of values of each parameter is equal to 2. Their algorithms focus on identifying the faulty tuples that have no more than 2 parameters.
%
%Ghandehari.etc \cite{ghandehari2012identifying} defines the suspiciousness of tuple and suspiciousness of the environment of a tuple. Based on this, they rank the possible tuples and generate the test cases. Although their approach imposes minimal assumption, it does not ensure that the tuples ranked in the top are the faulty tuples. They further utilized the test cases generated from the inducing interaction to locate the fault \cite{ghandehari2013fault}.
%
%Yilmaz \cite{yilmaz2006covering} proposed a machine learning method to identify inducing combinations from a combinatorial testing set. They construct a classified tree to analyze the covering arrays and detect potential faulty combinations. They \cite{yilmaz2006covering} additionally extend their work on variable covering array. Beside this, Fouch{\'e} \cite{fouche2009incremental} and Shakya \cite{shakya2012isolating} made some improvements in identifying failure-inducing combinations based on Yilmaz' work.
%
%Zhang \cite{zhang2012faulty} proposed an approach that are directly based on covering
%array. This approach translates MFS identification to a constraints satisfaction and optimal problem.

%Studies on symbolic execution, has .

%The second type of work focus on the code-level diagnosis in software, i.e., the fault localization techniques. Many methods

The second type of work  relates to the guaranteed code.

Elnatan \cite{reisner2010using} firstly used symbolic evaluation to analyse the configuration options in the program. In that work, they proposes the notion of guaranteed coverage, from which they find that the effective configuration space is relatively small when compared to the all the possible combinations of options. What's more, most coverage was accounted for by lower-strength interactions (2-way or 3-way), across all of line, basic block, edge, and conditions, but higher-strength interactions are needed for maximum coverage.  Based on this work, Charles \cite{song2012itree} proposed a test cases generation method, called iTree, which combines covering array and machine learning techniques to discover as more new coverage interactions as possible. Their experiments shows that their method is more effective than traditional CT and random methods at generating test cases with more coverage of program statements. They further refined their method iTree \cite{song2014itree} by re-constructing the composite interactions.

Our work differs from them in 1) we only focus on the guaranteed code of the MFS instead of all the possible interactions in the SUT. 2) we mainly studied the guaranteed code for fault localization instead of test case generation and program statements coverage.

\section{Conclusions and Future works}\label{sec:conclusion}
Combinatorial testing has been proven to be effective at detecting and identifying the failure-inducing interactions, i.e., MFS in the SUT. Most of their work focus on how to generate test cases and how to effectively identify the MFS. Few of them consider the relationship between MFS and code-level problem. In this paper, we studied the correlation between MFS and faulty code. Specifically, we obtain the weak and strong guaranteed code of MFS, and then compare them with faulty code to observe their relationship. Our empirical studies suggest that it do exists correlation between MFS and faulty code, but the extent to which of this relation depends on the faulty types and inputs modeling.

%\end{document}  % This is where a 'short' article might terminate

As a further work, we plan to use the conclusion in this paper to utilize MFS for code-level fault diagnosis. It is also appealing to study whether code-level fault diagnosis can optimal the inputs modeling of CT, according to the our 3rd empirical study. Besides them, we would like to conduct studies on more software programs with more types of faults to increase the generality of our work.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This work was supported by the National Natural Science Foundation of China (No. 61272079), the Research Fund for the Doctoral Program of Higher Education of China (No.20130091110032), the Science Fund for Creative Research Groups of the National Natural Science Foundation of China(No. 61321491), the Major Program of National Natural Science Foundation of China (No. 91318301), and National Science Foundation Award CCF-1464425.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\appendix
%Appendix A
%\section{Headings in Appendices}
%The rules about hierarchical headings discussed above for
%the body of the article are different in the appendices.
%In the \textbf{appendix} environment, the command
%\textbf{section} is used to
%indicate the start of each Appendix, with alphabetic order
%designation (i.e. the first is A, the second B, etc.) and
%a title (if you include one).  So, if you need
%hierarchical structure
%\textit{within} an Appendix, start with \textbf{subsection} as the
%highest level. Here is an outline of the body of this
%document in Appendix-appropriate form:
%\subsection{Introduction}
%\subsection{The Body of the Paper}
%\subsubsection{Type Changes and  Special Characters}
%\subsubsection{Math Equations}
%\paragraph{Inline (In-text) Equations}
%\paragraph{Display Equations}
%\subsubsection{Citations}
%\subsubsection{Tables}
%\subsubsection{Figures}
%\subsubsection{Theorem-like Constructs}
%\subsubsection*{A Caveat for the \TeX\ Expert}
%\subsection{Conclusions}
%\subsection{Acknowledgments}
%\subsection{Additional Authors}
%This section is inserted by \LaTeX; you do not insert it.
%You just add the names and information in the
%\texttt{{\char'134}additionalauthors} command at the start
%of the document.
%\subsection{References}
%Generated by bibtex from your ~.bib file.  Run latex,
%then bibtex, then latex twice (to resolve references)
%to create the ~.bbl file.  Insert that ~.bbl file into
%the .tex source file and comment out
%the command \texttt{{\char'134}thebibliography}.
%% This next section command marks the start of
%% Appendix B, and does not continue the present hierarchy
%\section{More Help for the Hardy}
%The sig-alternate.cls file itself is chock-full of succinct
%and helpful comments.  If you consider yourself a moderately
%experienced to expert user of \LaTeX, you may find reading
%it useful but please remember not to change it.
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
