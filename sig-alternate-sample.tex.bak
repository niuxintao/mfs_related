% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}

\usepackage{fancyvrb}
\usepackage{multirow}
\usepackage{threeparttable}

\hyphenpenalty=750

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Is MFS Strongly Correlated with faulty code?
\titlenote{(Produces the permission block, and
copyright information). For use with
SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{5} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Xintao Niu\\
       \affaddr{State Key Laboratory for Novel Software Technology}\\
       \affaddr{Nanjing University}\\
       \affaddr{China, 210023}\\
       \email{niuxintao@gmail.com}
%       \affaddr{changhainie@gmail.com}
% 2nd. author
%\alignauthor
%
%       \affaddr{State Key Laboratory for Novel Software Technology}\\
%       \affaddr{Nanjing University}\\
%       \affaddr{China, 210023}\\
%       \email{changhainie@nju.edu.cn}
% 3rd. author
\alignauthor
Laleh Sh. Ghandehari \\
       \affaddr{Department of Computer}\\
       \affaddr{Science and Engineering }\\
       \affaddr{The University of Texas at Arlington}\\
%       \affaddr{Arlington, Texas}\\
       \email{laleh.shikhgholamhosseing@mavs.uta.edu}
%       \email{ylei@cse.uta.edu}
 \and
\alignauthor
Changhai Nie\\
       \affaddr{State Key Laboratory for Novel Software Technology}\\
       \affaddr{Nanjing University}\\
       \affaddr{China, 210023}\\
       \email{changhainie@gmail.com}
\alignauthor
Jeff Lei \\
       \affaddr{Department of Computer}\\
       \affaddr{Science and Engineering }\\
       \affaddr{The University of Texas at Arlington}\\
%       \affaddr{Arlington, Texas}\\
       \email{ylei@cse.uta.edu}
\alignauthor
Xiaoyin Wang \\
       \affaddr{Department of Computer}\\
       \affaddr{Science}\\
       \affaddr{The University of }\\
       \affaddr{Texas at San Antonio}\\
 %      \affaddr{China, 211171}\\
%       \email{xujiaxi@njxzc.edu.cn} \\
       \email{Xiaoyin.Wang@utsa.edu}
 % use '\and' if you need 'another row' of author names
% \and
% 4th. author
%\alignauthor
%Hareton Leung \\
%       \affaddr{Department of computing}\\
%       \affaddr{Hong Kong Polytechnic University}\\
%       \affaddr{Kowloon, Hong Kong}\\
%       \email{hareton.leung@polyu.edu.hk}
%% 6th. author
%\alignauthor Charles Palmer\\
%       \affaddr{Palmer Research Laboratories}\\
%       \affaddr{8600 Datapoint Drive}\\
%       \affaddr{San Antonio, Texas 78229}\\
%       \email{cpalmer@prl.com}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
%\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Combinatorial Testing (CT) is an effective technique for testing the interactions of factors in the Software Under Test(SUT). Most works in CT focus on the technique itself, e.g., how to generate test cases, model the inputs, or handle the constraints of the inputs. Few works have considered the following question, i.e., is detecting and identifying the minimal failure-inducing interactions (MFS), really useful and helpful to code-level fault diagnosis? In this paper, we present the first study on the relationship between MFS and the code which causes the failure. Specifically, we firstly obtained the guaranteed code of the corresponding MFS, i.e., those program entities which are directly affected by these interactions. And then we compared these guaranteed code with those real faulty code to see whether there exists any associations between MFS with these real faulty code. Our empirical studies based on 7 programs from SRI showed that the correlation between failure-inducing interactions and the faulty code in the SUT is low to moderate, and this correlation is affected by the fault types and MFS  characteristics.

\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011102.10011103</concept_id>
<concept_desc>Software defect analysis~Software testing and debugging</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software defect analysis~Software testing and debugging}
%\ccsdesc{Reliability Verification~a}
%\ccsdesc[100]{Networks~Network reliability}

%
%\category{D.2.5}{Software Engineering}{Testing and debugging}[Debugging aids,testing tools]
%
%\terms{Reliability, Verification}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{Software Testing, Combinatorial Testing, Fault localization, Failure-inducing interactions, Guaranteed code}


\section{Introduction}
Modern software is becoming more and more complex. To test such software is challenging, as the candidate factors that can influence the system's behaviour, e.g., configuration options, system inputs, message events, are enormous. Even worse, the interactions between these factors can also cause runtime errors, e.g., the incompatibility problems. In consideration of the scale of the industrial software, to test all the possible interactions of all the factors (we call them the interaction space) is not feasible, and even if it is possible, it is not wise to test all the interactions because most of them do not provide any useful information.
%Further more, it is a standard . airline

Many empirical studies show that, in real software systems, the effective interaction space, i.e., targeting fault detection, makes up only a small proportion of the overall interaction space \cite{kuhn2002investigation,kuhn2004software}. What's more, the number of factors involved in these effective interactions is relatively small, of which 4 to 6 is usually the upper bounds\cite{kuhn2002investigation}. With this observation, applying Combinatorial testing(CT) in practice is appealing, as it is proven to be effective to detect the interaction faults in the system.

CT tests software with an elaborate test suite which checks all the required parameter value combinations, and after detecting some failures by this test suite, it then identify the failure-inducing interactions, or more formally, failure-causing schemas (MFS) in the SUT. Most works in CT focus on the method itself, e.g, to design smaller test suite with the same interaction coverage \cite{cohen1997aetg,cohen2003augmenting,lei2008ipog,jia2015learning}, or to identify the MFS more accurately \cite{martinez2009locating,nie2011minimal,zhang2011characterizing,niu2013identifying}. Few of the works consider the following question:

\emph{Is detecting and identifying the MFS really useful and helpful to code-level fault diagnosis?}

To analyse this question is important and necessary, because it builds the relationship between MFS and faulty code, which is the foundation to apply CT on code-level debugging. In this paper, we try to answer this question by studying the \emph{guaranteed code} of interactions. The \emph{guaranteed code} of a interaction is the program entities (e.g., statements, branches, blocks, etc.) which are directly \emph{affected} by the interaction according to the previous study \cite{reisner2010using}. Obtaining the guaranteed code of an interaction can help us understand how this interaction influence on the behaviour of the program under test. Furthermore, analysing the guaranteed code of the MFS can offer us an insight into the extent to which the MFS is related to the cause of the failure; based on which, we can learn whether detecting and identifying the MFS can facilitate the debugging, as well as bug fixing.

%To compute the MFS of the SUT,
%
%With respect to the guaranteed code.
%
%To compute the guaranteed code of the interaction, We need to utilize symbolic execution. Specifically, we accumulated added on symbolic, and find those guaranteed code. We define the .

%
There are many techniques to compute the MFS in CT. In this paper, we adopts the TRT method proposed in \cite{niu2013identifying}, which is an efficient and effective MFS identification technique in CT. With respect to guaranteed code, we follow the steps which are original proposed in \cite{reisner2010using}, which firstly utilizes symbolic execution tool to search possible paths for different values assigned to the input parameters, and then calculates the guaranteed code for each possible interaction based on these paths. One difference from study in \cite{reisner2010using} is that we only need to compute the guaranteed code for the MFS we identified in the SUT, instead of all the possible interactions. After obtaining the MFS and corresponding guaranteed codes, we are able to evaluate and analyze the correlation between MFS and faulty code.

We designed three empirical studies on 7 open-source software subjects from Software Infrastructure Repository (SIR) \cite{do2005supporting}. These studies considers several different aspects (e.g., the degree of MFS, the types of faults) of the relationships between MFS and faulty code. Our results suggests that: 1) The correlation between MFS  and the faulty code is weak to moderate; 2) There exists a significant decrease in the correlation between MFS and real faulty code with the increase of the degree MFS. 3) For different types of faults, the correlation between MFS and faulty code varies.
%3) The input model of the program under test significantly impacts on this correlation.

%These faults include injected and real ones. We first model their input and computed the MFS and also the guaranteed code of these MFS. We observe their relationships between these code with those real faulty statement. One observation in our case study shows that .  Besides this, we also compare . The results shows that .

%\textbf{Contributions:}

The remaindering of this paper are organised as follows: Section \ref{sec:pre} gives the preliminaries about Combinatorial testing (especially MFS-related), and basic definitions about Guaranteed code. Section \ref{sec:research} proposes three research questions that needs to be handled in this paper. Section \ref{sec:subjects} introduces the subjects on which our experiments are conducted on. Section \ref{sec:results} shows the results as well as the analysis. Section \ref{sec:related} discusses the related works.  Section \ref{sec:conclusion} concludes this paper.

\section{Preliminary and Formal Model}\label{sec:pre}
This section presents some formal descriptions about MFS and guaranteed code.

\subsection{Basic definitions about CT}\label{sec:pre:ct}

Assume that the Software Under Test (SUT) is influenced by \emph{n} parameters, and each parameter $p_{i}$ can take the values from the finite set $V_{i}$, $|V_{i}|$ = $a_{i}$ ($i$ = 1,2,..n). Then we give some basic definitions which are related to failure-inducing interactions in CT.

\newdef{definition}{Definition}
\begin{definition}
A \emph{test case} of the SUT is a tuple of \emph{n} values, one for each parameter of the SUT. It is denoted as  ($v_{1}$, $v_{2}$,...,$v_{n}$), where $v_{1}\in V_{1}$, $v_{2} \in V_{2}$ ... $v_{n} \in V_{n}$.
\end{definition}
%a \emph{n}-tuple

In practice, these parameters in the test case can represent many factors, such as input variables, run-time options, building options or various combination of them. We need to execute the SUT with these test cases to ensure the correctness of the behaviour of the SUT.

%\begin{definition}
%We consider any abnormally executing test case as a \emph{fault}. It can be a thrown exception, compilation error, assertion failure or constraint violation. When faults are triggered by some test cases, it is desired to figure out which parameter values in these test cases are the failure-inducing ones.
%Some subsets of these test cases should be analysed.
%\end{definition}


\begin{definition}
For the SUT, the \emph{n}-tuple (-,$v_{x_{1}}$,...,$v_{x_{k}}$,...)is called a \emph{k}-degree \emph{schema} ($0 < k \leq n $) when some k parameters have fixed values and other irrelevant parameters are represented as "-".
\end{definition}

For example, the tuple (-, 4, 4, -) is a 2-degree schema. In effect a test case itself is a k-degree \emph{schema}, when k = n. Furthermore, if a test case contains a \emph{schema}, i.e., every fixed value in the schema is in this test case, we say this test case \emph{contains} the \emph{schema}.
%, which can be denoted as $k-value\  combination \in T$

Note that the schema is a formal description of the interaction between parameter values we discussed before.

\begin{definition}
Let $c_{l}$ be a \emph{l}-degree schema, $c_{m}$ be an \emph{m}-degree schema in SUT and $l < m$. If all the fixed parameter values in $c_{l}$ are also in $c_{m}$, then $c_{m}$ \emph{subsumes} $c_{l}$. In this case we can also say that $c_{l}$ is a \emph{sub-schema} of $c_{m}$ and $c_{m}$ is a \emph{super-schema} of $c_{l}$, which can be denoted as $c_{l} \prec  c_{m}$.
\end{definition}

For example,  the 2-degree schema (-, 4, 4, -) is a sub-schema of the 3-degree schema (-, 4, 4, 5), that is, (-, 4, 4, -) $\prec$ (-, 4, 4, 5).

\begin{definition}
If all test cases that contain a schema, say $c$, trigger a particular fault, say $F$, then we call this schema $c$ the \emph{faulty schema} for $F$. Additionally, if none of sub-schema of $c$ is the \emph{faulty schema} for $F$, we then call the schema $c$ the \emph{minimal failure-causing schema (MFS)} \cite{nie2011minimal} for $F$.

%Based on this, if a test case $t$ hit such a failure-inducing combination, say $c(F)$, it should trigger the fault $F$, for which the test case can be put as $t(F)$
\end{definition}

Note that MFS is identical to the failure-inducing interaction discussed previously. In this paper, the terms \emph{failure-inducing interactions} and \emph{MFS} are used interchangeably.
%Figuring the MFS out helps to identify the root cause of a failure and thus facilitate the debugging process.

\subsection{An example of MFS}\label{sec:pre:mfs}
%When a test case fails during test case, we are still far from figuring out the MFS \cite{colbourn2008locating,martinez2008algorithms,martinez2009locating}, as we do not know exactly which schemas in the failed test cases should be responsible for the failure. In fact, for a failing test case ($v_{1},v_{2},...,v_{n}$), there can be at most $2^{n} - 1$ possible schemas for the MFS. Hence, more test cases should be generated to identify the MFS. The following example shows how MFS identification processes.

%For example, if test case (0, 0, 0, 0) failed during testing, there are six 2-degree candidate failure-inducing schemas, which are (0, 0, -, -), (0, -, 0, -), (0, -, -, 0) , (-, 0, 0, -), (-, 0, -, 0), (-, -, 0, 0), respectively. Without
%additional information, it is difficult to figure out the specific schemas in this suspicious set that caused the failure. Considering that the failure can be triggered by schemas with other degrees, e.g., (0, -, -, -) or (0, 0, 0, -), the problem of MFS identification becomes more complicated.



Consider an program like Fig \ref{toy-program}, which contains four integer parameters: a, b, c, and d. With respect to applying combinatorial testing, we need to first build a input model for this program. For simplicity, let a, b, c and d can take on the following values:  $v_{a} = \{ 1, 2\}$, $v_{b} = \{ 0, 1\}$, $v_{c} = \{ 1, 2\}$, and $v_{d} = \{ 0, 1\}$, respectively. Assume that through testing of this program, we find a test case, e.g., (a = 1, b = 0, c = 1, d = 1), will trigger an arithmetic exception. Then we will describe how CT identify the MFS in this test case.

%taking square root of a negative number
\begin{figure}
\begin{Verbatim}[numbers=left,xleftmargin=5mm]
public float foo(int a, int b, int c, int d){
  int x, y, z;
  x = 4;
  if(a < 2){
    y = 3;
    if(c < 2){
            z =  Math.sqrt(y - x);
            return z / y;
    }
    else
        return x + y;
  }
  else{
    y = 2;
    if(b < 1){
        if(d < 1)
            return y / (x + y);
        else
            return x / (x + y);
    }
    else
        return y * x;
  }
}
\end{Verbatim}
\caption{A simple program \emph{foo} with four input parameters}
\label{toy-program}
\end{figure}

A typical MFS identification process is shown in Table \ref{ofot-identify}. In this table, test case $t$ represents the failing test case we aforementioned. To identify the MFS, we mutate one factor of \emph{t} one time to generate new test cases: $t_{1}$ -- $t_{4}$.  It turns out that test case $t_{1}$ passed, which indicates that this test case break the MFS in the original test case \emph{t}. So (1, -, -, -) should be a failure-causing factor. Similarly, we can also conclude that (-, -, 1, -) is another failure-inducing factor because of the pass of $t_{3}$. Considering that all the other test cases failed, which means no other failure-inducing factors were broken, therefore, the MFS in \emph{t} is (1, -, 1, -).

\begin{table}[ht]
\caption{OFOT example}
\label{ofot-identify}
\center
\begin{tabular}{llllll}
 \hline
\multicolumn{5}{c}{\bfseries Original test case} & \bfseries Outcome \\  \hline
 $t$ & \multicolumn{4}{l}{1 \ \ \ \ 0 \ \ \ \  1 \ \ \ \ 1 } & Fail \\
 \hline
\multicolumn{5}{c}{\bfseries Additional  test cases} &  \\  \hline
$t_{1}$ &\multicolumn{4}{l}{2  \ \ \ \  0 \ \ \ \  1 \ \ \ \ 1 }& Pass \\
$t_{2}$ &\multicolumn{4}{l}{1  \ \ \ \  1 \ \ \ \  1 \ \ \ \ 1 } & Fail \\
$t_{3}$ &\multicolumn{4}{l}{1  \ \ \ \  0 \ \ \ \  2 \ \ \ \ 1 } & Pass \\
$t_{4}$ &\multicolumn{4}{l}{1  \ \ \ \  0 \ \ \ \  1 \ \ \ \ 0 } & Fail \\
 \hline
\end{tabular}
\end{table}

This identification process mutate one factor of the original test case at a time to generate extra test cases. Then according to the outcome of the test cases execution result, it identifies the MFS of the original failing test cases. It is called the OFOT method \cite{nie2011minimal}, which is a well-known MFS identification method in CT.
%It should be noted that the following proposed CT framework can be easily applied to other MFS identification methods.


\subsection{Formal description about guaranteed code}\label{sec:pre:guar}
Although MFS are the failure-inducing parts of the failing test input for the SUT, however, we still cannot directly utilize it for fault localization, because they do not provide any code-level information. For this, we need to build the relationship between input schemas and program entities.

Let $\mathcal{P}$ be a path in the SUT. Then let $Cov(\mathcal{P})$ = $< s_{1}, s_{2}, s_{3},... s_{n} >$ be the program entities that covered by path $\mathcal{P}$. A program entity can be a statement, block, edges, etc. In this paper, we mainly focus on statements; note that other types of structure coverage can also be applied \cite{song2012itree}. Let $Pcon(\mathcal{P})$ = $<pc_{1}, pc_{2}, pc_{3}, ... pc_{k}>$ be the path conditions that are encountered by path $\mathcal{P}$.  As an example, consider the program list in Fig \ref{toy-program}. It is easy to find that it has five possible paths, which forms the execution tree in Fig \ref{foo-tree}.

\begin{figure}
 \includegraphics[width=3.4in]{foo_tree.eps}
\caption{The execution tree of program \emph{foo}}
\label{foo-tree}
\end{figure}

In this tree, a rhombus node represents a path condition, and a rectangle represents the statement. Note that those consecutive statements are included in one rectangle. From this figure, we can list the paths, their covered entities, and their path conditions, which are explicitly shown in Table \ref{cov-con-foo}.

\begin{table}[ht]
\caption{Paths, covered entities, and conditions of \emph{foo}}
\label{cov-con-foo}
\center
\begin{tabular}{lll}
 \hline
\bfseries ID & \bfseries Covered Entities & \bfseries Path Conditions \\  \hline
 $1$ & S2, S3, S5, S7, S8& $a < 2$,  $c < 2$\\
 $2$ & S2, S3, S5, S11 & $a < 2$,  $\neg (c < 2)$\\
 $3$ & S2, S3, S14, S17& $\neg (a < 2)$,  $b < 1$, $d < 1$\\
 $4$ & S2, S3, S14, S19& $\neg (a < 2)$,  $b < 1$, $\neg (d < 1)$\\
 $5$ & S2, S3, S14, S22& $\neg (a < 2)$,  $\neg (b < 1)$\\
 \hline
\end{tabular}
\end{table}


Next we give some important definitions that are related to the guaranteed code.
%For simplicity, we next use notation $c^\sharp$ indicates a logic expression meaning that the parameter values are equal to those in schema $c$.

\begin{definition}
For a schema $c$, and a path $\mathcal{P}$, if all the path conditions in this path can be satisfied when given an input that contains this schema, we call $\mathcal{P}$ a \emph{Consistent path} of schema $c$, which can be denoted as $SAT(Pcon(\mathcal{P}) \bigwedge c)$.
\end{definition}

In fact, to judge whether a path is a \emph{consistent path} of one schema, is to find whether exist an input that contains this schema and follows all the path conditions of this path. Taking the program \emph{foo} for example, Path 1 is a consistent path of schema (1, -, -, -), because $( a < 2 $ $\wedge $ $c < 2 $ $\wedge $ $a = 1 )$  $= true $ can be solved, e.g., input (1, 1, 1, 1) is such a result.

Based on this definition, we use the notation $c^{\sharp}$ to represent the set of all the consistent paths of schema $c$. Then we give the formal definition of weak guaranteed of one schema.

\begin{definition}
For a schema $c$, the weak guaranteed code of $c$, i.e., $wg(c)$, is the intersection of program entities covered by its consistent paths. Formally, $wg(c)$ = $\bigcap_{\mathcal{P} \in c^{\sharp}} Cov(\mathcal{P})$.

\end{definition}

To better illustrate the weak guaranteed code of schemas, we list some schemas in the program \emph{foo}, their consistent paths and weak guaranteed code in Table \ref{weg-foo}. For example, for schema (1, -, -, -), its consistent paths are Path 1 and Path 2, because we can find inputs that contain this schema and satisfy all the path conditions of them. For example, (1, 1, 1, 1) can go through Path 1 and (1, 1, 2, 1) can go through Path 2. These two inputs both contain the schema of (1, -, -, -). Hence, the weak guaranteed code of $(1, -, -, -)$ are the intersection of the entities covered by these two paths, i.e., S2, S3, and S5.

Note that there is one special schema (-, -, -, -), listed in this Table, it is the sub-schemas of all the other schemas of program \emph{foo}. What's more, it is consistent with all the paths of program \emph{foo}, and hence its weak guaranteed code are S2 and S3, which are the common code of all the paths.

\begin{table}[ht]
\caption{Weak guaranteed code of some schemas of \emph{foo}}
\label{weg-foo}
\center
\begin{tabular}{lll}
 \hline
\bfseries Schema & \bfseries $c^{\sharp}$ & \bfseries $wg(c)$\\  \hline
 $(1, -, -, -)$ & Path 1, 2& S2, S3, S5 \\
 $(1, -, 1, -)$ & Path 1 & S2, S3, S5, S7, S8\\
 $(1, -, 2, -)$ & Path 2 & S2, S3, S5, S11\\
 $(2, -, -, -)$ & Path 3, 4, 5& S2, S3, S14\\
 $(2, 0, -, -)$ & Path 3, 4& S2, S3, S14\\
 $(2, 0, -, 1)$ & Path 4& S2, S3, S14, S19\\
 $(2, 1, -, -)$ & Path 5& S2, S3, S14, S22\\
 $(-, -, 1, -)$ & Path 1, 3, 4, 5 & S2, S3 \\
 $(-, -, -, -)$ & Path 1, 2, 3, 4, 5 & S2, S3 \\
 \hline
\end{tabular}
\end{table}

Above all, we can observe that, the weak guaranteed code\footnote{The definition of weak guaranteed code is similar to the \emph{guarantee coverage} defined in \cite{reisner2010using}. The difference is that in this paper we do not distinguish the input and value symbolic; instead, they are handled in an unified way.} of one schema, essentially, is the maximal common code that is expected to be executed by any path which consists with this schema. In other word, this schema guarantees some code to be executed.

%\footnote{Weak guaranteed is ,
%Strong guaranteed is Note that this is the same to the interactions in paper. To let different, we }.

%Note that the definitions is one different from original defined, . which we do not consider input, instead, they are also modeled as symbolic. Based on This, all the input can be handled unified as the same way as those symbolic values.



Although the weak guaranteed code always follows with the corresponding schema, it does not necessarily indicates that the schema directly controls the weak guaranteed code. This is because, for one schema, there may exists some other schemas that may always appears with this schema. Hence, it may result in that some code are the weak guaranteed code of one schema, but these code may be actually controlled by other schemas. It is easy to learn that these ``accompanying'' schemas are the sub-schemas of one schema.

For example, with respect to program \emph{foo}, schema (1, -, 1, -) always appears with schema (1, -, -, -), hence, the weak  guaranteed code of schema (1, -, 1, -), i.e., (S2, S3, S5, S7, S8) must always contain the weak guaranteed code of schema (1, -, -, -), i.e., (S2, S3, S5).
More formally, we can conclude the relationship of the weak guaranteed code between subsuming schemas as the following proposition.

\newtheorem{proposition}{Proposition}
\begin{proposition}
Given schemas $s_{1}$, $s_{2}$, where $s_{1} \prec s_{2}$, then $wg(s_{1}) \subseteqq wg(s_{2})$.
\end{proposition}

It is easy to prove this proposition and hence we omit it here. Based on this proposition, to understand which code are under the direct control of some schemas, we need to remove the influence from their subschemas.
%Considering this, it may in fact the behaviour under the sub-schema . For example, in Fig.  Hence, to totally reflect the influence behaviour of sub-schemas, we need to remove the impact from the sub-schemas.

\begin{definition}
For a schema $c$, the strong guaranteed code of $c$, i.e., $sg(c)$, is the weak guaranteed code of $c$ with removing those weak guaranteed code of its subschemas. Formally, $sg(c)$ = $wg(c) \backslash \{\bigcup_{c' \prec c} wg(c') \}$.

\end{definition}

Strong guaranteed code are the program entities that under the direct control of the corresponding schema, and hence it reflects how the schema influence on the behaviour of program. As an example, considering the schemas which we show their weak guaranteed code in Table \ref{weg-foo}. We list the weak guaranteed code of all their subschemas, and the strong guaranteed code of themselves in Table \ref{strong-foo}. For example, for schema (1, -, 1, -), its weak guaranteed code are (S2, S3, S5, S7, S8), while the weak guaranteed code of all its subschemas are: (S2, S3, S5), (S2, S3), (S2, S3) for schemas (1, -, -, -), (-, -, 1, -) and (-, -, -, -), respectively. Hence, the union of the weak guaranteed code of its subschemas are (S2, S3, S5), and the strong guaranteed code of (1, -, 1, -) are (S7, S8).

\begin{table}[ht]
\caption{Strong guaranteed code of some schemas of \emph{foo}}
\label{strong-foo}
\center
\begin{tabular}{lll}
 \hline
\bfseries Schema & \bfseries $wg\ of\ subschemas$ & \bfseries $sg(c)$\\  \hline
 $(1, -, -, -)$ & S2, S3 & S5 \\
 $(1, -, 1, -)$ & S2, S3, S5 & S7, S8\\
 $(1, -, 2, -)$ & S2, S3, S5 & S11\\
 $(2, -, -, -)$ & S2, S3 &  S14\\
 $(2, 0, -, -)$ & S2, S3, S14 &  - \\
 $(2, 0, -, 1)$ & S2, S3, S14 & S19\\
 $(2, 1, -, -)$ & S2, S3, S14 & S22\\
 $(-, -, 1, -)$ & S2, S3 & - \\
 $(-, -, -, -)$ & - & S2, S3 \\
 \hline
\end{tabular}
\end{table}


In this paper, we focus on the guaranteed code of MFS, instead of all the other schemas in the test case. With respect to the example in Fig \ref{toy-program}, it is easily to find that the weak guaranteed code and strong guaranteed code of the MFS (1, -, 1, -) are, (S2, S3, S5, S7, S8), and (S7, S8),  respectively. They are correlated to the faulty code, i.e., S7, where an exception of taking square root of a negative number will be triggered. This example, especially for the strong guaranteed code, implies that MFS is closely related to the faulty code.  However, in the real situation, we do not know whether the conclusion can still hold. As we want to know whether to detect and identify the MFS is really helpful in the code-based diagnosis, hence, we need to do more empirical studies to verify the conclusion.


\section{Research questions} \label{sec:research}
To comprehensively study the correlativeness between the MFS and faulty code, we propose three research questions in the following.

\subsection{The correlation between MFS and faulty code}
As discussed before, it is important to study the relationship between MFS and faulty code. Although the simple example in Section \ref{sec:pre:guar} shows there exists a strong correlation between the guaranteed code , it is necessary to verify it on more real program subjects. Hence, it motivates our first research question, which is also the key reteach question, that is :

\textbf{Q1: Is the guaranteed code of the MFS correlated to the faulty code in real program subjects?}

To answer this question, we need to conduct studies on a batch of program subjects. In these studies, the MFS, their weak and strong guaranteed code, and the fault code should be investigated and analyzed. Another point that needs to note is how we evaluate the correlation between the guaranteed code and faulty code. Inspired by the ranking formula that is used in fault localization \cite{naish2011model,abreu2007accuracy}, in this paper, we adopts the following  formula to compute the relevance between two codes, i.e.,
 \begin{equation}\begin{aligned}\label{eq:metric}
 Correlation (A, B)= \frac{Common(A, B)}{Common(A, B) + Different(A, B)} .\end{aligned} \end{equation}

Here, $Common(A, B)$ means the number of common statements between two code blocks A and B, and $Differen$ $t (A, B)$ means the number of different statements contained in either  A or B. Based on this formula, more number of common code indicates a closer correlation, while more number of different code, on the contrary, indicates a more distinct or irrelative relationship.


\subsection{The influence of the degree of MFS}
The second research question is raised from the degree of the MFS. Based on the definition of guaranteed code, it is easy to learn that, with the increase of the degree of the MFS, the number of lines of weak guaranteed code also increases. This because the MFS with less degree may have more compatible paths that can contain this schema. Take the example in Section \ref{sec:pre}, 1-degree schema (1, - , -, -  )have two compatible paths (Path 1, 2), while 2-degree (1, -, 1, -) only have one path (Path 1). As result, to obtain the weak guaranteed code, the MFS with less degree need to conduct more intersection between the lines of code of paths, and hence it may decrease the number of lines of code for the weak guaranteed code. However, more number of lines may have a negative effect on the correlation between guaranteed code with real faulty code (Considering that there may exists much more different lines of code between them).

Hence, the second research question is:

\textbf{Q2: To what extent does the degree of MFS affect the correlation between it and faulty code?}


To answer question, we need to classify the MFS into different groups according to their degrees, and then observe whether there exists significant differences among these groups with respect to the correlation between their guaranteed code with real faulty code.

% the number of blocks that we still need to inspect until we hit the faulty code. Considering we only offer one possible statement, other statements are all depend on their execution sequence. If there is many, we only obtain the sentect that hit the failure, and its distance.

\subsection{The influence of different types of faults}

According to the nature of the defect, e.g., missing construct, or wrong construct, faults can be categorized into many types. The influence of different type of fault varies widely; as a consequence, the MFS and corresponding guaranteed code of different type of fault may also varies. Hence, focusing on single type of fault may significantly impact on the generality of our study about the correlation between MFS and faulty code, which motivates the last research question:

\textbf{Q3: To which extent does different type of fault influence on the correlation between MFS and faulty code ?}

To answer this question, we need first give the the characterization and classification of software faults. According to the study \cite{duraes2006emulation}, we decide to conduct experiments on the fault types listed in Table \ref{tab_fault typles}. These faults are classified according to the three ODC \cite{chillarege1996orthogonal} classes, i.e., Assignment, Checking, and Interface faults. Note that we have omit two more types of faults which are originally listed in \cite{duraes2006emulation}, as those two types of faults are related to the design or requirement faults. For each of these faults, they are refined into three sub-types, which are based on the nature of the defects, i.e., the fault caused by missing construct, wrong construct, or superfluous part. The column ``Example'' in Table \ref{tab_fault typles} shows some samples of the corresponding type of fault.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[!ht]
  \centering
  \caption{Fault types according to defect nature and ODC class}
    \begin{tabular}{|c|c|c|}  \hline
    ODC class & Nature & Example \\ \hline
    Assignment & missing & A variable was not assigned a value, a variable was not initialized, etc. \\
          & wrong & A wrong value (or expression result, etc) was assigned to a variable \\
          & extraneous & A variable should not have been subject of an assignment \\  \hline
    Checking & missing & An "if" construct is missing, part of a logical condition is missing, etc. \\
          & wrong & Wrong logical expression used in a condition in branch and loop construct. \\
          & extraneous & An "if" construct is superfluous and should not be present \\  \hline
    Interface & missing & A parameter in a function call was missing; incomplete expression as used as parameter \\
          & wrong & Wrong information was passed to a function call (value, expression result, etc.) \\
          & extraneous & Surplus data is passed to a function \\  \hline
    \end{tabular}%
  \label{tab_fault typles}%
\end{table*}%

Based on the categories given in Table \ref{tab_fault typles}, we next study these faults and their corresponding MFS to observe whether there exists any distinction among different types.


%\subsection{The influence of inputs model}
%The last research question is raised from CT itself. It is known that inputs modeling, as the key part of CT \cite{nie2011survey}, significantly influence on the result of CT. For example, if we use the following inputs modeling : $v_{a} = \{ 0, 1 \}$, $v_{b} = \{ 0, 1\}$, $v_{c} = \{ 1, 2\}$, and $v_{d} = \{ 0, 1\}$, respectively, for the program \emph{foo} in Fig \ref{toy-program} instead of what is originally used in Section \ref{sec:pre}. Then it is easy to compute that MFS of \emph{foo} is (-, -, 1, -), which is different from the original MFS (1, -, 1, -). We can learn the new MFS is irrelevant to the parameter $a$; this is because according to the execution tree in Fig \ref{foo-tree}, no matter what $a$ is assigned to ($0$ or $1$), it can only follow $Path 1$ or $Path 2$, and hence cannot execute other paths. As a result, the fault is only depended on what $c$ is assigned to. The changes of MFS also impacts on the guaranteed code. From Table \ref{weg-foo} and \ref{strong-foo}, the weak guaranteed code of (-, -, 1, -) is $S2$ and $S3$, while there is no strong guaranteed code. This results shows that the correlation between MFS and the real faulty code ($S7$) is  very trivial, which is different from the conclusion which is based on the original inputs modeling.
%
%Considering this, the third research question is:
%
%\textbf{Q3: How does the inputs model of CT affect the correlation between MFS and faulty code?}
%
%
%To answer question, for each program subject in our empirical studies, we design 3 different inputs model (they vary in the the number of parameters and the number of values for each parameter). Then we will obtain their MFS, guaranteed code, and the correlation between MFS and faulty code, respectively. At last, we will evaluate whether there exist any difference among these results.

%These questions are important, as

\section{Subject programs}\label{sec:subjects}
We have prepared 7 program subjects for our experiment, which are belong to the Siemens set \cite{hutchins1994experiments}. We obtain these subjects, as well as their program specifications, from the Software Infrastructure Repository (SIR) \cite{do2005supporting}. These subjects have been wildly used in the studies of fault detection and localization \cite{zeller2002isolating,jones2005empirical,renieres2003fault,ghandehari2013fault}.  Table \ref{subjects_Chaar} shows the specific program subjects, number of lines of code, the brief introduction, and the number faulty versions for each subject.

%. Note that althouth , but they contain many.
%The Siemens programs perform a variety of tasks: tcas is an aircraft collision avoidance system, schedule2 and schedule are priority schedulers, totinfo computes statistics given input data, printtokens and printtokens2 are lexical analyzers, and replace performs pattern matching and substitution.

\begin{table}[!ht]
  \centering
  \caption{Characteristics of subject programs}
    \begin{tabular}{|c|c|c|c|}  \hline
    Subject & Loc & Description & Versions \\ \hline
    printtokens & 472 & lexical analyzers   &  7 \\
    printtokens2 & 399 & -   &  10 \\
    replace & 512 & pattern substitution   &  32 \\
    schedule & 292 & priority scheduler   &  9 \\
    schedule2 & 301 & -   &  10 \\
    tcas &  141 & altitude separation   &  41 \\
    totinfo & 440  & information measure   &  23 \\ \hline
  %  grep & 10068  & pattern recognition   &  19 \\  \hline
    \end{tabular}%
  \label{subjects_Chaar}%
\end{table}%


\subsection{Fault characteristics}
For each program subject in our experiment, there are several faulty versions come with the correct version. These faults vary in different types and locations.  With respect to research question 3, we need to classify these faults into 3 main types and each of them has 3-sub types. Therefore, we checked the source file for each version of the specific programs, compared them with the correct version, and then classified the faults according to the aforementioned types. Table \ref{subject fault} shows the fault type distribution for each subject.
%Using mutation to inject faults into.
\begin{table*}[!ht]
  \centering
  \caption{Fault type distribution of subject programs}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}  \hline
    \multirow{2}{*}{Subjects} & \multicolumn{3}{c|}{Assignment} & \multicolumn{3}{c|}{Checking} & \multicolumn{3}{c|}{Interface} \\ \cline{2-10}
     & missing &  wrong   &  extraneous & missing &  wrong   &  extraneous & missing &  wrong   &  extraneous \\ \hline
    printtokens & 2 &   2  & 1 & 0 &   1  & 1 &  1&   0  & 0   \\
    printtokens2 & 1 &  2  & 0 & 2 &  1   & 3 & 0 &  1   & 0  \\
    replace & 1 &  9  & 1 & 10 &  8   & 4 & 0 &  3   & 0  \\
    schedule &  1 & 2  & 0 & 1 &   4  & 1 & 0  & 0    & 0  \\
    schedule2 &  1 &  1    & 0 & 5  &  0   & 2 & 0 &  1   & 0   \\
    tcas & 0 &  40  & 2 &  1 &   1   & 0  & 0 &  0   & 0 \\
    totinfo & 0 &  9  &0 &1  & 8    &3  & 1 & 1    & 0   \\ \hline
   % grep & 0 &  0  &6 &  0&   0  & 13 &  0&   0  &  0 \\  \hline
    \end{tabular}%
  \label{subject fault}%
\end{table*}%

Note that there may exist multiple types of faults in one subject. For example, the fault version 1 of subject \emph{printtokens} has two types of faults, which are belong to missing code of variable assignment and extraneous code of variable assignment, respectively. As a result, the total number of the faulty types of the programs with all the versions is large than the number of faulty versions of the programs.

\subsection{Input modeling}

To test each subject program, we need to model their inputs space firstly. Specifically, We followed the instructions described in \cite{ghandehari2013applying} to build the corresponding model for each subject, which include defining the key parameters and the values of each parameter for the programs, obtaining the possible constraints amon these parameters. For example, for the program \emph{replace} in the Siemens suite, we may consider there parameters: pattern, substitute and input text. Each of them have different type of value, e.g., the pattern may be a common character, digit, or line matching signal, etc. Table \ref{subjects_inputs} listed these models, the number of test cases, and the number of constraints of each subject. The model is presented in the abbreviated form $\#values^{\#num\ of\ param} \times ...$. For example, in Table \ref{subjects_inputs}, schedule subject, $2^9 \times 3^2$ means that there are 9 parameters with 2 possible values, and 2 parameters with 2 possible values.

%The detailed model is also available for review in. The model column in the table shows the number of parameters and their domain size. We represent it by where d indicates that there are k number of parameters with domain size as d. Note that, which is the total number of parameters
\begin{table}[ht]
  \centering
  \caption{Inputs modeling of subject programs}
    \begin{tabular}{|c|c|c|c|}  \hline
    Subject & Model & Tests & Cons \\ \hline
    printtokens & $2^{5} \times 3^{1} \times 4 ^{2} \times 5 ^{2} $&   38400  &  8 \\
    printtokens2 & $2^{7} \times 3 ^{2} \times 4 ^{1}  $ &   4608   & 8 \\
    replace & $2^{11}$  &  2048 &  36\\
    schedule &  $2^{9} \times 3 ^{2}$& 4608 & 0\\
    schedule2 & $2^{9} \times 3 ^{2}$ & 4608  & 0 \\
    tcas & $2 ^{7} \times 3^{2} \times 4 ^{1} \times 10 ^{2}$  & 460860  & 0\\
    totinfo & $3 ^{3} \times 5 ^{2} \times 6^{1}$   & 4050  & 0 \\ \hline
  %  grep & $2 ^{7} \times 4 ^{1} \times  6 ^{5} \times 9 ^{1} \times 13 ^{1}$  &    &  2 \\  \hline
    \end{tabular}%
  \label{subjects_inputs}%
\end{table}%

%\subsection{Obtaining traces}
%%
%Run all the possible test cases, and obtain their traces. Such that we can compute the guaranteed code of each MFS.

%\section{Getting MFS and their guaranteed code}

\section{Results}\label{sec:results}
In this section, we display and analyse the results of our experiments, based on which we will answer the three research questions posed in Section \ref{sec:research}.

\subsection{The characteristics of the MFS}
We firstly give the characteristics of the MFS.  These MFS are identified by the tool \emph{TRT}\cite{niu2013identifying}.
Table \ref{subject mfs} shows the distribution of the MFS with various degree for each subject.

\begin{table*}[!ht]
  \centering
  \caption{The degree of the mfs for the subject programs}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}  \hline
    \multirow{2}{*}{Subjects} & \multicolumn{12}{c|}{The number of MFS with specific degree} &  \multirow{2}{*}{Total}\\ \cline{2-13}
        & 1 &  2   &  3 & 4 &  5   &  6 & 7 &  8   &  9  & 10 & 11 & 12 &  \\ \hline
    printtokens & 0 &   0  & 8 & 0 &   0  & 0 &  0&   0  & 0 & 0 &0 & 0 & 8  \\
    printtokens2 & 0 &   11 & 8 & 0 &   0  & 0 &  0&   0  & 0 & 0 &0 & 0 & 19 \\
    replace & 0 &   11  & 8 & 9 &   120  & 133 &  472 &   1135  & 2883 & 0 &0 & 0 & 4671 \\
    schedule & 0 &   14  & 20 & 10 &   0  & 0 &  0&   0  & 0 & 0 &0 & 0 & 44  \\
    schedule2 & 0 &   0  & 0 & 0 &   132 & 187 &  523&   1139  & 0 & 0 &0 & 0 & 1981   \\
    tcas & 0 &   0  & 0 & 0 &   0  & 76 &  465&   1130  & 2882 & 3057 &1274 & 132 & 9076  \\
    totinfo & 0 &   0  & 0 & 8 &   110  & 114 &  0&   0  & 0 & 0 &0 & 0 & 232 \\ \hline
  %  grep & 0 &   0  & 0 & 0 &   0  & 0 &  0&   0  & 0 & 0 &0 & 0 & 0\\  \hline
    \end{tabular}%
  \label{subject mfs}%
\end{table*}%

We can learn that the degree of most MFS is less than 6, which is in accordance with the observation in the study \cite{kuhn2002investigation}. One exception is tcas, we found that most of them are greater than 6 (up to 12).
%With inspecting the code of tcas, we believe the cause of this result is the \emph{Coincidental Correctness} problem \cite{masri2014prevalence}, i.e., the fault is triggered by a relative small number (1 to 6) of parameter values, but it may not be observed because it cannot propagate to the output. There exist many restrictions that make this fault observable, and hence the degree of the MFS increases.
Another observation from Table \ref{subject mfs} is that there may exists multiple MFS for a single faulty version. Hence, the total number of MFS is far greater than the number of faulty versions.

Next, we will show the average number of lines of the guaranteed code (weak and strong) for the MFS with different degrees in Table \ref{mfs wk_numberlines}.

\begin{table*}[!ht]
  \centering
  \caption{The average number of lines of guaranteed code of the mfs}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}  \hline
    & \multirow{2}{*}{Subjects} & \multicolumn{12}{c|}{The average number of lines code of the MFS with specific degree} \\ \cline{3-14}
    &    & 1 &  2   &  3 & 4 &  5   &  6 & 7 &  8   &  9  & 10 & 11 & 12  \\ \hline
   \multirow{8}{*}{\rotatebox{90}{Weak}} & printtokens & - &   -  & 123.9 & - &   -  & - &  - & - &- & -&- & -    \\
   & printtokens2 & - &  116.5  & 123.8 & - &   -  & - &  - & - &- & -&- & -  \\
   & replace & - &   116.6  & 123.9 & 46.0 &   58.3  & 69.9 &  53.4 & 51.2 & 50.6 & -&- & -  \\
   & schedule & - &   108.3  & 96.4 & 49.2 &   -  & - &  - & - &- & -&- & -  \\
   & schedule2 & - &   -  & - & - &  60.6  & 73.7 &  56.3 & 51.3 &- & -&- & -     \\
   & tcas &   -  & - & - &- & -  & 51.1 &  51.5  & 50.6 & 50.6 &  50.9  & 51.5 &  52.5  \\
   & totinfo & - &   -  & - & 29.5 &   47.7  & 52.4 &  -&   -  & - & - &- & -  \\ \hline
 %  & grep & - &   -  & - & - &   -  & - &  - & - &- & -&- & -  \\  \hline
  \multirow{8}{*}{\rotatebox{90}{Strong}}  & printtokens & - &   -  & - & 0.0 &   -  & - &  - & - &- & -&- & -    \\
   & printtokens2 & - &   5.1  & 0.0 & - &   -  & - &  - & - &- & -&- & -  \\
   & replace & - &   5.1  & 0.0 & 7.1 &   26.3 & 5.5 &  0.0 & 0.1 &0.2 & -&- & - \\
   & schedule & - &   4.0  & 0.0  & 6.4 &   -  & - &  - & - &- & -&- & -  \\
   & schedule2 & - &   -  & - & - &   23.9  & 3.9 &  0.0 &  0.1 &- & -&- & -    \\
   & tcas &   -  & - & - &- & -  & 0.0 &  0.0  & 0.1 & 0.2 &  0.2  & 0.1 &  0.0  \\
   & totinfo & - &   -  & - & 8.0 &   28.6  & 6.5 &  - & - &- & -&- & -  \\ \hline
   %& grep & - &   -  & - & - &   -  & - &  - & - &- & -&- & -  \\  \hline
    \end{tabular}%
  \label{mfs wk_numberlines}%
\end{table*}%

One observation of Table \ref{mfs wk_numberlines} is that the number of lines of weak guaranteed code is far more than that of strong guaranteed code. It is easy to understand, as according to the definition, we need to remove many lines of code from weak guaranteed to obtain the strong guaranteed.

%Another point that we can learn from this table is that, with the increase of the degree of the MFS, the number of lines of weak guaranteed code also increases. This result matches our analysis in Section \ref{sec:research}.

\subsection{The correlation between MFS and real faulty code}
We first utilized the GNU tool \emph{diff} to compare the correct and faulty programs. We mark the differences between them as the real faulty code, i.e., the line numbers of different code in the faulty versions. Then we used these real faulty code to evaluate the accurateness of the guaranteed code (weak and strong) of those identified MFS according to the Formula \ref{eq:metric} in Section \ref{sec:research}. The results are listed in Fig \ref{result1_fig}. Each sub-figure of Fig \ref{result1_fig} shows the correlations between real faulty code and the guaranteed code (Weak and Strong) for one program subject.

\begin{figure*}[!ht]
 \includegraphics[width=6.8in]{result1.eps}
\caption{The correlation between guaranteed code with real faulty code for each subject}
\label{result1_fig}
\end{figure*}

From this figure, we can first observe that for different program subjects, the correlation between guaranteed code and real faulty code varies. For example, with respect to weak guaranteed code, its correlation with real faulty code is around 0.02 for \emph{tcas}, while it reaches about 0.04 for \emph{totinfo}. With respect to strong guaranteed code, the result is ranged from 0.05 to 0.25 for \emph{totinfo}, while for \emph{tcas}, this result is 0 for all the MFS.


Second, the correlation of strong guaranteed code is either greater than that of weak guaranteed code, or alternatively, is equal to 0 (which indicates that it has non correlation with the real faulty code). For example, it reaches 0.25 for \emph{totinfo}, which means that we only need to inspect at most four program lines on average to obtain one real faulty code. This result is relatively effective for fault localization, and is far greater than the correlation of its weak guaranteed code, which is about 0.02 to 0.04.  While for subjects \emph{schedule}, {schedule2}, and \emph{tcas}, all the results for the correlation of strong guaranteed code is 0.  This observation implies that,  when compared to the weak guaranteed code, the strong guaranteed code of the MFS may remove some program entities that are exactly the real faulty statements. As a result, the correlation between the strong guaranteed code and real faulty code may be 0.

The last, and the most important observation of this figure is that, for most subjects, the correlation between guaranteed code and real faulty code is low to moderate (Most of them are around  0.01, 0.02 and even 0). It implies that the MFS cannot be directly used in most cases with respect to code-level fault localization. By inspecting the programs in our subjects, we conclude two reasons that cause the negativeness results:

1) The guaranteed code is large (containing too many program entities that are non-faulty). We believe the cause of this result is the \emph{Coincidental Correctness} problem \cite{masri2014prevalence}, i.e., the fault is triggered by a relative small number (1 to 6) of parameter values, but it may not be observed because it cannot propagate to the output. As a result, there exist many restrictions that make this fault observable, and hence the MFS contain many factors that are not related to the fault itself but related to how to make the fault observable.

2) Multiple faults in the program. This will result in that, for one MFS, its consistent paths may actually trigger different faults from each other. Hence, the intersection of the code of its consistent paths, i.e., the weak guaranteed code, may not contain the code of any fault (Note that it happens in the condition that there is no common code between different faults).

%One exception is tcas, of which the correlation of weak guaranteed code is greater than that of strong guaranteed code. In fact all the result of strong guaranteed code is 0, indicating that there is no correlation between them.


%With inspecting the code of tcas,

% many program problem, makes the fault hard to observe, . In fact, when we find that wrong strong code, we can learn it is the last , which is under the last part of the control path with this code. That is, it is on same \emph{slicing}. We believe for programs like tcas, it is good to combine program slicing to obtain the real faulty code.

Above all, the answer for the research question 1 is :

\textbf{The correlation between MFS and real faulty code is low to moderate, and some factors, i.e., coincidental correctness, and multiple faults, may negatively affect the result.}


\subsection{MFS degree}
The second experiment is to evaluate whether there exists any difference, with respect to the correlation between real faulty code and guaranteed code, in the MFS with different degrees. For this purpose, we classified the results in the first experiment according to 3 types of degrees, which are the MFS with degrees of 1 to 3, with degrees of 4 to 6, and with degrees more than 6, respectively.  Additionally, to determine whether there exists significant difference between those three types, we conduct t-test on each pair of them. The results are shown in Table \ref{degree_significance}. Note that a p-value smaller than 0.05 indicates that the performance of these two approaches are statistically different with each other at 95\% confidence. We can learn that most results are statistically significant.
\begin{figure}[!ht]
 \includegraphics[width=3.4in]{result3.eps}
\caption{The correlation between guaranteed code with real faulty code for different degree of MFS}
\label{result3_fig}
\end{figure}

According to Fig \ref{result3_fig}, it is clear that the MFS with degrees ranging from 4 to 6 have the highest correlation with real faulty code, following by the MFS with other two types of degrees. This result applies to both weak guaranteed code and strong guaranteed code. More specifically, when compared to the MFS with degree ranged from 1 to 3 and MFS with degree more than 6, there is a decrease about 15\% in the correlation of weak guaranteed code, and 100\% decrease in the correlation of strong guaranteed code (Note that the correlation is 0 for both other two types), respectively.

There are two implications that we can learn from this observation:

1) If the MFS has a high degree, it seems that the useful information that is related to the real fault makes up only a small proportion of its guaranteed code. An intuitive explanation for this result is that a MFS with high degree may contain some \emph{noisy} factors, i.e., these parameter values may not be related to the fault, but always appears with it. Additionally, the guaranteed code of MFS with high degree may contain a large number code lines (See Table \ref{mfs wk_numberlines}), which may have a negative effect on the score of correlation between it with real faulty code (The latter contain only a small number of lines in our program subjects).

2) On the other hand, if the MFS has a low degree, its guaranteed code may not contain the real faulty code. This is because the MFS with low degree may have many consistent paths. When these consistent paths diff greatly from each other (For example, when there are different faults in these paths), the intersection of their covered lines probably do not contain the faulty code.

%The second is that, with respect to ODC class. the is best (around ), followed by  (), the last is . It is easy to understand, because. While with respect to the defect natures, we can learn that the is best (around ), followed by  (), the last is . For this, we believe it is because.
Hence, the answer for the research question 2 is :

\textbf{The MFS with moderate degrees (4 to 6) have the highest correlation with real faulty code, when compared to the MFS with other degrees.}

\begin{table}[!ht]
\centering
\caption{The t-test result for different MFS degree}
\begin{threeparttable}
  \centering
    \begin{tabular}{|c|c|c|c|}  \hline
 \multirow{2}{*}{weak} &  P(x, y)\tnote{1} & P(x, z) & P(y, z) \\ \cline{2-4}
  &  2.0E-17 & 6.3E-22 & 2.7E-23   \\ \hline
  \multirow{2}{*}{Strong}&  P(x, y) & P(x, z) & P(y, z) \\ \cline{2-4}
  &  5.0E-11 & NaN & 4.9E-11 \\ \hline
    \end{tabular}%

       \begin{tablenotes}
        \footnotesize
        \item[1]  $p(m,n)$ denotes the p-value of a test of significance for the probability that the results of two metrics, i.e., m and n, are equal, where m, n can be x = the results of MFS with degree 1 to 3, y = MFS with degree 4 to 6, and z = MFS with degree more than 6.
      \end{tablenotes}

\end{threeparttable}
  \label{degree_significance}%
\end{table}%

\subsection{Fault types}
In the last experiment, we investigate whether the correlation between faulty code and guaranteed code is affected by various fault types. Therefore, we separated the results in the first experiment according to different ODC classes and defect classes (include missing code type, wrong code type, and extraneous code type). The results are listed in Figure \ref{result2_fig}, where the left part shows the results of three ODC classes, and the right part shows that of three defect classes. Similar to the second experiment, to determine whether there exists significant difference between those metrics, we conduct t-test on each pair of the metrics in ODC, and defect classes, respectively. The results are shown in Table \ref{type_significance}.
%Note that a p-value smaller than 0.05 indicates that the performance of these two approaches are statistically different with each other at 95\% confidence.
%Note that statistically significant.

\begin{figure*}[!ht]
 \includegraphics[width=6.8in]{result2.eps}
\caption{The correlation between guaranteed code with real faulty code for different fault type}
\label{result2_fig}
\end{figure*}

\begin{table}[!ht]
\centering
\caption{The t-test result for different fault types}
\begin{threeparttable}
  \centering
    \begin{tabular}{|c|c|c|c|c|}  \hline
 \multirow{4}{*}{\rotatebox{90}{weak}} &\multirow{2}{*}{ODC} &  P(a, c)\tnote{1} & P(a, i) & P(c, i) \\ \cline{3-5}
  &  & 0.49  & 4.3E-6  &  1.3E-5   \\ \cline{2-5}
  & \multirow{2}{*}{Defect} &  P(m, w) & P(m, e) & P(w, e) \\ \cline{3-5}
  &  & 1.3E-8 & -& -  \\ \hline
  \multirow{4}{*}{\rotatebox{90}{Strong}} & \multirow{2}{*}{ODC} &  P(a, c) & P(a, i) & P(c, i) \\ \cline{3-5}
  &  & 0.15 & 7.6E-7 & 2.4E-7   \\ \cline{2-5}
  & \multirow{2}{*}{Defect} &  P(m, w) & P(m, e) & P(w, e) \\ \cline{3-5}
  &  &0.26 & -&    -\\ \hline
    \end{tabular}%

       \begin{tablenotes}
        \footnotesize
        \item[1]  a = assignment, c = checking, i = interface, m= missing code, w = wrong code, and e = extraneous code.
      \end{tablenotes}

\end{threeparttable}
  \label{type_significance}%
\end{table}%

According to Fig \ref{result2_fig},  we can find that the results vary among different metrics. Specifically, for the \emph{ODC} class, it shows that the \emph{Interface} type has the highest correlation on the strong guaranteed code (most of them are around 0.25), following by type \emph{Checking} (about 0.02 to 0.04), and \emph{Assignment}  (around 0.02). Note that although there are some points of \emph{Checking} and \emph{Assignment} reach to 0.35, these points makes up a tiny proportion of all the MFS with that degree.  The weak guaranteed code has a similar result for these three metrics. We believe the cause of this is that, the \emph{Interface} type fault is simple, as it only relates to one or two invocation statements in the program, and hence, it is easy to be distinguished and isolated from other program statements. While for the \emph{Assignment} and \emph{Checking} types, these faults may combine other other statements (Logic or assignment statements), which results in that they are harder to be distinguished from other statements.

With respect to the defect classes, we can observe that the \emph{Wrong code} type has the highest correlation on both strongly (0 to 0.045) and weakly guaranteed code (0.01 to 0.04). The \emph{Missing code} type followed (Note that some correlation of \emph{Missing code} can reach 0.25, but these faults are all belong to \emph{Interface} class, all the others are around 0.01). Here we omit the discussion of \emph{extraneous fault} type because the number of faults that belong to this type is too small to produce a statistically significant conclusion.

We believe the reason why because type \emph{Wrong code} has a higher correlation is because that, for \emph{Wrong code} type, there exists specific lines of code that can trigger the failure. While for type \emph{Missing code}, although we can compare the faulty version with correct version to obtain the location where the correct code is missing, however, there may exist many other locations where the correct code can be inserted, such that it can also be an equivalent version of the original correct program. As a result, the guaranteed code of MFS with type \emph{Missing code} has very limited correlation with the real fault.

Hence, the answer for the research question 3 is :

\textbf{For different types of faults, the correlation between MFS and faulty code varies; More specifically, the faults that are belong to \emph{Interface} class and \emph{Wrong Code} type have a higher correlation than that of other types of faults.}

%
% has no means to do with ODC class. only with types.
%
%the faulty code is, wrong the code, extra the code, missing two codes nearest it.
%
%
%The conclusion is, that the wrong code and extra code are more close, because those which control this is more close to introducing faulty.
%
%While the missing is very. This is because some code may be .

%\subsection{Inputs modeling }
%The last experiment is to evaluate the extent to which that inputs model affect the correlation between MFS and faulty code.  Table \ref{subject degree_model} lists the change on the degree of identified MFS for two additional inputs model (Model 2 and Model 3 in Table \ref{subjects_inputs}). In this table, column \emph{Average Degree}
%
%\begin{table*}[!ht]
%  \centering
%  \caption{The degree of the mfs for different input modeling}
%    \begin{tabular}{|c|c|c|c|c|c|c|}  \hline
%   \multirow{2}{*}{Subjects}   & \multicolumn{3}{c|}{2rd Input Model} &  \multicolumn{3}{c|}{3rd Input Model}   \\ \cline{2-7}
%        & Average Degree &  Change   &  P-value  & Average Degree &  Change   &  P-value   \\ \hline
%    printtokens & 0 &   0  & 0 & 0 &   0  & 0 \\
%    printtokens2  & 0 &   0  & 0 & 0 &   0  & 0 \\
%    replace  & 0 &   0  & 0 & 0 &   0  & 0  \\
%    schedule  & 0 &   0  & 0 & 0 &   0  & 0  \\
%    schedule2 & 0 &   0  & 0 & 0 &   0  & 0    \\
%    tcas & 8.17&-1.23&6.4E-62 &7.27&-2.14 &1.01E-38   \\
%    totinfo  & 0 &   0  & 0 & 0 &   0  & 0  \\
%    grep & 0 &   0  & 0 & 0 &   0  & 0 \\  \hline
%    \end{tabular}%
%  \label{subject degree_model}%
%\end{table*}%
%
%We can learn that there is a clear decrease in the degree for the two inputs models when compared to the original model (Model 1 in Table \ref{subjects_inputs}). Note that the p-value shows that this decrease is significant. This result shows that with the decrease of number of parameters in the inputs model, the degree of the MFS may also decrease because some of the failure-inducing factors may be eliminated from the inputs model.
%
%Next,
%
%
%\begin{table*}[!ht]
%  \centering
%  \caption{The correlation between guaranteed code and real faulty code for different input modeling}
%    \begin{tabular}{|c|c|c|c|c|c|c|c|}  \hline
%     &  \multirow{2}{*}{Subjects}   & \multicolumn{3}{c|}{2rd Input Model} &  \multicolumn{3}{c|}{3rd Input Model}   \\ \cline{3-8}
%   \multirow{8}{*}{\rotatebox{90}{Weak guaranteed}}   &   & Average Correlation &  Change   &  P-value  & Average Correlation &  Change   &  P-value   \\ \cline{2-8}
%   &  printtokens & 0 &   0  & 0 & 0 &   0  & 0 \\
%   &  printtokens2  & 0 &   0  & 0 & 0 &   0  & 0 \\
%   &  replace  & 0 &   0  & 0 & 0 &   0  & 0  \\
%   &  schedule  & 0 &   0  & 0 & 0 &   0  & 0  \\
%   &  schedule2 & 0 &   0  & 0 & 0 &   0  & 0    \\
%  &   tcas & 0.02&6.9E-4&0.35 & 0.02&1.9E-3&0.14  \\
%  &   totinfo  & 0 &   0  & 0 & 0 &   0  & 0 \\
%   &  grep & 0 &   0  & 0 & 0 &   0  & 0 \\  \hline
%   \multirow{8}{*}{\rotatebox{90}{Strong guaranteed}}   &   & Average Correlation &  Change   &  P-value  & Average Correlation &  Change   &  P-value   \\ \cline{2-8}
%  &   printtokens & 0 &   0  & 0 & 0 &   0  & 0 \\
%  &   printtokens2  & 0 &   0  & 0 & 0 &   0  & 0 \\
%  &   replace  & 0 &   0  & 0 & 0 &   0  & 0  \\
%  &   schedule  & 0 &   0  & 0 & 0 &   0  & 0  \\
%  &   schedule2 & 0 &   0  & 0 & 0 &   0  & 0    \\
%  &   tcas & 0& 0 & NaN  & 0&0&NaN  \\
%  &   totinfo  & 0 &   0  & 0 & 0 &   0  & 0 \\
% &    grep & 0 &   0  & 0 & 0 &   0  & 0 \\  \hline
%    \end{tabular}%
%  \label{subject degree_correlate}%
%\end{table*}%
%
%(Note that in some pictures, the condition coverage cannot be 100 percent because some exception will not be triggered in the current inputs, e.g., the input memory exception handle module will not be triggered)
%
%Above all, the answer to the question 3 is :
%
%\textbf{The more the coverage the inputs modeling supports, the more accurate the MFS is correlated to the faulty code.}

%type assign check  assign check 
%

%

\subsection{Threats to Validity}

There are several threats to validity in our empirical studies.

First, our experiments are based on only 7 open-source software, of which the program scale is small or medium sized. More subject programs are desired to make the results more general.

Second, we only applied one inputs modeling \cite{ghandehari2013applying} for our program subjects. Other inputs modeling may result in different results, e.g., the overall coverage of the programs with these models, and the characteristics of the MFS, may be different with different models.

At last, we should balance the number of different types of faults. In fact, based on Table \ref{subject fault}, there is none extraneous code type in the odc class \emph{Interface}. We should introduce more faults of such type to avoid deviation.
%For example, it is appealing to study the multiple faults in one program, or faults that are related to more than one statements.
%\section{Discussion}\label{sec:discuss}
%Based on the results of our empirical studies, we can conclude that in most cases, we cannot directly use the MFS into defect analyzing. Only in some special . However, this is based on the . When consider other information, it seems. We have take such a . For tcas, 23 46 in their  slicing of failing test cases. It seems



\section{Related works}\label{sec:related}

There are several works that are related to our study, and they can be categorised into two types:

The first type of methods studies the MFS in CT. Nie \cite{nie2011minimal} firstly studied the properties of the MFS in SUT, based on which additional test cases were generated to identify them. Other approaches to identify the MFS in SUT include building a tree model \cite{yilmaz2006covering}, adaptively generating additional test cases according to the outcome of the last test case \cite{zhang2011characterizing}, ranking suspicious interactions based on some rules \cite{ghandehari2012identifying}, and using graphic-based deduction \cite{martinez2008algorithms}, among others. These approaches can be partitioned into two categories \cite{colbourn2008locating} according to how the additional test cases are generated: \emph{adaptive}--additional test cases are chosen based on the outcomes of the executed tests \cite{shi2005software,nie2011minimal,ghandehari2012identifying,niu2013identifying,zhang2011characterizing,shakya2012isolating,wang2010adaptive,li2012improved}or \emph{nonadaptive}--additional test cases are chosen independently and can be executed in parallel \cite{yilmaz2006covering,colbourn2008locating,martinez2008algorithms,martinez2009locating,zhang2012faulty}. All these works focused on input-level or configuration-level testing, i.e., their target is to identify the failure-inducing inputs or options, not the code in the program.

Besides these work, there exists two studies that focus on utilizing MFS to locate the faulty statement in the program \cite{ma2013locating,ghandehari2013fault}. The first work \cite{ma2013locating} adopted OFOT \cite{nie2011minimal} to calculate the MFS, and then compares the additional generated test cases with the original failing test cases to locate the failure-causing chains and corresponding statements. The second work \cite{ghandehari2013fault} took the ranking method \cite{ghandehari2012identifying} to identify the MFS. Then  it generated additional passing test cases based on the MFS, and ranked the suspicious statements by comparing the spectrum of the failing test case and passing test cases.

Our work differs from the first type of work in that we focus on both MFS and faulty code, not just any single object. Additionally, our work does not specifically describes how to identify MFS or how to conduct code-level fault localization, instead, we studied the correlation between them. We believe this is important and will give a guideline for how to utilize MFS for fault diagnosis.

%Combinatorial testing has been widely applied in industry \cite{kuhn2010practical}. Fault localization is an important part of CT studies. \cite{nie2011survey}, as it can facilitate debugging efforts by reducing the scope of code that is needed for inspection \cite{ghandehari2012identifying}.
%
%Shi and Nie \cite{shi2005software} presented an approach for failure revealing and failure diagnosis in CT , which first tests the SUT with a covering array, then reduces the value schemas contained in the failing test case by eliminating those appearing in the
%passing test cases.
%
%Nie's approach in \cite {nie2011minimal} first separates the faulty-possible tuples and healthy-possible tuples into two sets. Subsequently, by changing one parameter value at a time of the original test case, this approach generates extra test cases. After executing the configurations, the approach converges by reducing the number of tuples in the faulty-possible sets. Wang then \cite{wang2010adaptive} extended this approach by adaptively mutates multiple parameter values instead of only one factor at a time.
%
%Delta debugging \cite{zeller2002simplifying} proposed by Zeller is an adaptive divide-and-conquer approach to locate interaction fault. It is very efficient and has been applied to real-life software environment. Zhang et al. \cite{zhang2011characterizing} also proposed a similar approach that can identify the failure-inducing combinations that have no overlapped part efficiently.  JieLi proposed a similar approach \cite{li2012improved}.
%
%%Charles \cite{colbourn2008locating} proposed,
%
% Charles \cite{colbourn2008locating} proposed the notion of ($d$,$t$)-detecting array, which corresponds to test cases that permit the determination of MFS in the SUT, if the number of MFS $d$ and degree of MFS $t$ are known in prior.  C. Martiez \cite{martinez2008algorithms} also proposed a non-adaptive method. Their approach extends the covering array to the locating array to detect and locate interaction faults. They \cite{martinez2009locating} then extend this non-adaptive method to any degree with the theory of hyber-graphic.  C. Martiez \cite{martinez2008algorithms,martinez2009locating}proposed two adaptive algorithms. The first one needs safe value as their assumption and the second one remove this assumption when the number of values of each parameter is equal to 2. Their algorithms focus on identifying the faulty tuples that have no more than 2 parameters.
%
%Ghandehari.etc \cite{ghandehari2012identifying} defines the suspiciousness of tuple and suspiciousness of the environment of a tuple. Based on this, they rank the possible tuples and generate the test cases. Although their approach imposes minimal assumption, it does not ensure that the tuples ranked in the top are the faulty tuples. They further utilized the test cases generated from the inducing interaction to locate the fault \cite{ghandehari2013fault}.
%
%Yilmaz \cite{yilmaz2006covering} proposed a machine learning method to identify inducing combinations from a combinatorial testing set. They construct a classified tree to analyze the covering arrays and detect potential faulty combinations. They \cite{yilmaz2006covering} additionally extend their work on variable covering array. Beside this, Fouch{\'e} \cite{fouche2009incremental} and Shakya \cite{shakya2012isolating} made some improvements in identifying failure-inducing combinations based on Yilmaz' work.
%
%Zhang \cite{zhang2012faulty} proposed an approach that are directly based on covering
%array. This approach translates MFS identification to a constraints satisfaction and optimal problem.

%Studies on symbolic execution, has .

%The second type of work focus on the code-level diagnosis in software, i.e., the fault localization techniques. Many methods

The second type of work  relates to the guaranteed code.

Elnatan \cite{reisner2010using} firstly used symbolic evaluation to analyse the configuration options in the program. In that work, they proposes the notion of guaranteed coverage, from which they find that the effective configuration space is relatively small when compared to the all the possible combinations of options. What's more, most coverage was accounted for by lower-strength interactions (2-way or 3-way), across all of line, basic block, edge, and conditions, but higher-strength interactions are needed for maximum coverage.  Based on this work, Charles \cite{song2012itree} proposed a test cases generation method, called iTree, which combines covering array and machine learning techniques to discover as more new coverage interactions as possible. Their experiments shows that their method is more effective than traditional CT and random methods at generating test cases with more coverage of program statements. They further refined their method iTree \cite{song2014itree} by re-constructing the composite interactions.

Our work differs from them in 1) we only focus on the guaranteed code of the MFS instead of all the possible interactions in the SUT. 2) we mainly studied the guaranteed code for fault localization instead of test case generation and program statements coverage.

\section{Conclusions and Future works}\label{sec:conclusion}
Combinatorial testing has been proven to be effective at detecting and identifying the failure-inducing interactions, i.e., MFS in the SUT. Most of their work focus on how to generate test cases and how to effectively identify the MFS. Few of them consider the relationship between MFS and code-level problem. In this paper, we studied the correlation between MFS and faulty code. Specifically, we obtain the weak and strong guaranteed code of MFS, and then compare them with faulty code to observe their relationship. Our empirical studies suggest that it do exists correlation between MFS and faulty code, but the extent to which of this relation depends on the faulty types and inputs modeling.

%\end{document}  % This is where a 'short' article might terminate

As a further work, we plan to use the conclusion in this paper to utilize MFS for code-level fault diagnosis. It is also appealing to study whether code-level fault diagnosis can optimal the inputs modeling of CT, according to the our 3rd empirical study. Besides them, we would like to conduct studies on more software programs with more types of faults to increase the generality of our work.

%ACKNOWLEDGMENTS are optional
%\section{Acknowledgments}
%This work was supported by the National Natural Science Foundation of China (No. 61272079), the Major Program of National Natural Science Foundation of China (No. 91318301), and National Science Foundation Award CCF-1464425.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\appendix
%Appendix A
%\section{Headings in Appendices}
%The rules about hierarchical headings discussed above for
%the body of the article are different in the appendices.
%In the \textbf{appendix} environment, the command
%\textbf{section} is used to
%indicate the start of each Appendix, with alphabetic order
%designation (i.e. the first is A, the second B, etc.) and
%a title (if you include one).  So, if you need
%hierarchical structure
%\textit{within} an Appendix, start with \textbf{subsection} as the
%highest level. Here is an outline of the body of this
%document in Appendix-appropriate form:
%\subsection{Introduction}
%\subsection{The Body of the Paper}
%\subsubsection{Type Changes and  Special Characters}
%\subsubsection{Math Equations}
%\paragraph{Inline (In-text) Equations}
%\paragraph{Display Equations}
%\subsubsection{Citations}
%\subsubsection{Tables}
%\subsubsection{Figures}
%\subsubsection{Theorem-like Constructs}
%\subsubsection*{A Caveat for the \TeX\ Expert}
%\subsection{Conclusions}
%\subsection{Acknowledgments}
%\subsection{Additional Authors}
%This section is inserted by \LaTeX; you do not insert it.
%You just add the names and information in the
%\texttt{{\char'134}additionalauthors} command at the start
%of the document.
%\subsection{References}
%Generated by bibtex from your ~.bib file.  Run latex,
%then bibtex, then latex twice (to resolve references)
%to create the ~.bbl file.  Insert that ~.bbl file into
%the .tex source file and comment out
%the command \texttt{{\char'134}thebibliography}.
%% This next section command marks the start of
%% Appendix B, and does not continue the present hierarchy
%\section{More Help for the Hardy}
%The sig-alternate.cls file itself is chock-full of succinct
%and helpful comments.  If you consider yourself a moderately
%experienced to expert user of \LaTeX, you may find reading
%it useful but please remember not to change it.
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
